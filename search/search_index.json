{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DIVE Documentation","text":"<p>This is the documentation site for DIVE, a free and open-source annotation and analysis platform for web and desktop built by Kitware. DIVE integrates with the VIAME toolkit, but it can also be used on its own.</p> <p> Try the web version  Get the desktop app  Get support</p> <p></p>"},{"location":"#feature-comparison","title":"Feature Comparison","text":"Web Desktop Data Load your own images and videos \u2714\ufe0f \u2714\ufe0f \u00a0\u00a0\u00a0 Image and video transcoding \u2714\ufe0f \u2714\ufe0f \u00a0\u00a0\u00a0 Import using image lists \u274c \u2714\ufe0f Load annotations from supported formats \u2714\ufe0f \u2714\ufe0f Create new object and track annotation \u2714\ufe0f \u2714\ufe0f Annotation export \u2714\ufe0f \u2714\ufe0f Dataset export for portability between web and desktop \u2714\ufe0f \u2714\ufe0f Permissions and sharing support for team collaboration \u2714\ufe0f \u274c Annotation Bounding boxes \u2714\ufe0f \u2714\ufe0f Polygons \u2714\ufe0f \u2714\ufe0f Head/Tail lines \u2714\ufe0f \u2714\ufe0f Linear interpolation \u2714\ufe0f \u2714\ufe0f Track split \u2714\ufe0f \u2714\ufe0f Multi-way track merge \u2714\ufe0f \u2714\ufe0f Complex Interactions and activity groups \u2714\ufe0f \u2714\ufe0f Freeform or multi-select attributes \u2714\ufe0f \u2714\ufe0f Data Review Image enhancement (thresholding) \u2714\ufe0f \u2714\ufe0f Advanced per-type annotation confidence threshoding \u2714\ufe0f \u2714\ufe0f Review save history and load previous states \u2714\ufe0f \u274c VIAME Integration Run VIAME object detection and tracking \u2714\ufe0f \u2714\ufe0f Run VIAME detector and tracker training \u2714\ufe0f \u2714\ufe0f VIAME multi-camera pipelines \u274c \u2714\ufe0f Manual refinement of auto-generated annotations \u2714\ufe0f \u2714\ufe0f"},{"location":"#concepts-and-definitions","title":"Concepts and Definitions","text":"<p>DIVE is the annotator and data management software system.  It is our name for the code and capabilities, including both web and desktop, that can be deployed and configured for a variety of needs.</p> <p>VIAME (Video and Image Analytics for Marine Environments) is a suite of computer vision tools for object detection, tracking, rapid model generation, and many other types of analysis.  Get more info at viametoolkit.org</p> <p>VIAME Web is the specific DIVE Web deployment at viame.kitware.com. It includes a web-based annotator with the capabilities to run VIAME workflows on user-provided data.  You may deploy the web system into your own lab or cloud environment.</p> <p>Detection - A single annotation.  A detection could be associated with a point in time within a track, or it could have no temporal association.</p> <p>Features - Bounding box, polygon, head/tail points or other visible elements of a detection.</p> <p>Track - A collection of detections spanned over multiple frames in a video or image sequence.  Tracks include a start and end time and can have gap periods in which no detections exist.</p> <p>Group - A collection of one or more tracks, which can be given a definite frame range, type annotation, confidence, and attributes.</p> <p>Types - Every track (or detection, if tracks aren't applicable) has one or more types that should be used to annotate the primary characteristic you are interested in classifying.  Types are typically used to train a single or multi-class classifier.  A track (or detection) may have multiple types with confidence values associated.</p> <p>Frame - A single image or point in time for a video or image sequence.</p> <p>Key Frame - Every manually drawn annotation is considered a keyframe, and all automated pipelines produce keyframes. Only keyframes can have attributes.  Key frame detections are differentiated from interpolated detections, which are the implicit bounding boxes you see when linear interpolation is enabled.</p> <p>Interpolation - The implicit bounding boxes between keyframes in a track.</p> <p>Attributes - Attributes are free-form secondary characteristics on both tracks and detections. For example, a <code>fish</code> type track may have an <code>is_adult</code> boolean attribute.</p>"},{"location":"Annotation-QuickStart/","title":"Annotation Quickstart","text":"<p>Before following the quickstart, it could be helpful to skim the User Interface Guide</p>"},{"location":"Annotation-QuickStart/#single-frame-detections","title":"Single Frame Detections","text":"<p>How to quickly create multiple detections on a single image frame.</p> <ol> <li>Click  (creation settings menu) in the Track List area.<ol> <li>From the Mode dropdown, choose Detection.</li> <li>From the Type dropdown, choose or enter a default name that all new detections will have.  If the type doesn't exist yet, enter a name to create a new one.</li> <li>Turn on the Continuous Mode switch if you would like to automatically re-enter the creation state so you can click-and-drag repeatedly to quickly create many detections.</li> </ol> </li> <li>Enter the annotation creation state by clicking  Detection or pressing the N key.</li> <li>Create your first detection by clicking and dragging to draw a rectangle.</li> <li>If you are in continuous mode, click and drag again to create the next detection.<ol> <li>Press Esc to exit continuous creation mode.</li> </ol> </li> </ol>"},{"location":"Annotation-QuickStart/#single-detection-mode-demo","title":"Single Detection Mode Demo","text":"<p>The demo below shows how to use Detection mode to quickly create numerous detections of the same type.</p> <p></p>"},{"location":"Annotation-QuickStart/#track-annotations","title":"Track Annotations","text":"<p>How to quickly create track annotations for a video or image sequence.</p>"},{"location":"Annotation-QuickStart/#interpolation-mode","title":"Interpolation Mode","text":"<p>Linear interpolation is a kind of spatio-temporal annotation that allows the inference of bounding boxes between keyframes.  Interpolation mode is the fastest and easiest way to generate track annotations.</p> <p>Interpolation editing for existing tracks will only be enabled on tracks that span more than one frame. It is enabled on new tracks by default.</p> <ol> <li>Click  (creation settings menu) in the Track List area.<ol> <li>From the Mode dropdown, choose Track.</li> <li>Also ensure that the Interpolate switch is turned on.</li> </ol> </li> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>Create your first detection by clicking and dragging to draw a rectangle around the object you want to track.</li> <li>You can now go forward one or more frames by pressing F or Right or by using the Timeline controls and an outline of the previous annotation will remain.<ol> <li></li> </ol> </li> <li>To set another keyframe, either move or resize the transparent annotation or press K. There are also controls on for the currently selected track to add/remove keyframes. <ol> <li> and  will allow you to add and remove the current keyframe.</li> <li> and  will turn on or off interpolation for the current keyframe interval region you are in.</li> <li></li> </ol> </li> </ol>"},{"location":"Annotation-QuickStart/#visualizing-interpolated-tracks","title":"Visualizing interpolated tracks","text":"<p>Click Events in the Timeline controls to see where interpolation occurs and where the keyframes are located.</p> <ul> <li>Keyframes are indicated by solid rectangular blue tick marks in the highlighted track.</li> <li>Interpolated regions are indicated by a thin yellow line between keyframes.</li> <li>Gap regions are indicated by areas with neither interpolated frames nor keyframes.  Typically means that a track is off-camera or occluded.</li> </ul> <p></p>"},{"location":"Annotation-QuickStart/#interpolation-mode-demo","title":"Interpolation Mode Demo","text":""},{"location":"Annotation-QuickStart/#advance-frame-mode","title":"Advance Frame Mode","text":"<p>This mode keeps you editing the same track while automatically advancing the frame each time a detection is drawn.  In most cases interpolation mode will be easier.</p> <ol> <li>Click  (creation settings menu) in the Track List area.<ol> <li>From the Mode dropdown, choose Track.</li> <li>Also ensure that the Interpolate switch is turned off.</li> </ol> </li> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>Create your first detection by clicking and dragging to draw a rectangle around the object you want to track.</li> <li>Now each time an individual detection is drawn the frame will automatically advance to the next frame.  Press Esc to end creation of the track.</li> </ol>"},{"location":"Annotation-QuickStart/#advance-frame-mode-demo","title":"Advance Frame Mode Demo","text":"<p>The demo below shows how to use AdvanceFrame mode to travel through the video while creating annotations.</p> <p></p>"},{"location":"Annotation-QuickStart/#head-tail-annotations","title":"Head Tail Annotations","text":""},{"location":"Annotation-QuickStart/#adding-headtail-points-to-existing-annotations","title":"Adding Head/Tail points to existing annotations","text":"<ol> <li>Right-click an existing detection to enter edit mode.</li> <li>Enter head/tail creation mode<ol> <li>In the Edit bar, click </li> <li>Or Press H to create a head point.</li> <li>Or press T to create a tail point.</li> </ol> </li> <li>The mouse cursor will become a crosshair.  Click in the annotator to place each point.</li> <li>Once the first marker is placed it automatically transitions to the second marker. If you start with head, the second one will be the tail and vice versa.</li> </ol>"},{"location":"Annotation-QuickStart/#creating-new-annotations-using-headtail-points","title":"Creating new annotations using Head/Tail points","text":"<p>You can create a track by starting with a head/tail annotation or just a single point.</p> <ol> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>In the Edit bar, click  to switch to head/tail creation mode or press H, T, or 3.</li> <li>The mouse cursor will become a crosshair.  Click in the annotator to place each point.</li> <li>Press Esc to finish creation after one or both points have been placed.</li> </ol>"},{"location":"Annotation-QuickStart/#other-notes-on-headtail","title":"Other notes on Head/Tail","text":"<ul> <li> <p>The head point is denoted by a filled circle, while the tail point is denoted by a hollow circle.</p> <p></p> </li> <li> <p>You don't have to place both markers.  Press Esc on your keyboard at anytime to exit out of the line creation mode.</p> </li> <li>You can modify an existing head/tail marker by placing the annotation into 'Edit Mode' and then selecting the line tool from the editing options.</li> <li>You can delete a head/tail pair by selecting a detection with existing markers, entering edit mode, and clicking Delete Linestring </li> </ul>"},{"location":"Annotation-QuickStart/#fish-head-tail-demo","title":"Fish Head Tail Demo","text":""},{"location":"Annotation-QuickStart/#polygon-annotations","title":"Polygon Annotations","text":"<p>Every track is required to have a bounding box, but a polygon region may be added.  When a polygon is created or edited it will generate or adjust the bounding box to fit the size of the polygon.</p>"},{"location":"Annotation-QuickStart/#polygon-creation","title":"Polygon Creation","text":"<ol> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>In the Edit bar, click  or press 2 to enter polygon creation mode.</li> <li>Place each point on the polygon by clicking.</li> <li>Right-Click to automatically close the polygon or press Esc to cancel creation.</li> </ol>"},{"location":"Annotation-QuickStart/#polygon-editing","title":"Polygon Editing","text":"<ol> <li>Right click an annotation to enter edit mode.</li> <li>In the Edit bar, click  or press 2 to enter polygon edit mode.</li> <li>Click and drag any large circle handle to move it.  This will move the point to a new position and recalculate the bounding box.</li> <li>Click and drag any small circle handle to create new points. This can be used to adjust the polygon and make it appear smoother.</li> <li>To delete the whole polygon, in the Edit bar, click Del polygon </li> <li>To delete a single keypoint, click its handle then click Del Point N </li> </ol>"},{"location":"Annotation-QuickStart/#polygon-demo","title":"Polygon Demo","text":""},{"location":"Annotation-User-Interface-Overview/","title":"User Interface Guide Introduction","text":"<p>This documentation section provides a reference guide to the annotation interface organized by screen region.</p> <p></p> <ul> <li>Navigation and Editing Bar - Controls to return back to browser as well as perform higher level functions such as running pipelines. Save Button.  Controls the viewing of annotations on screen and allows for the editing/creation of annotations.</li> <li>Annotation View - where the image/video is displayed as well as all annotations</li> <li>Type List - A list of all the types of tracks/detections on the page that can be used to filter the current view.</li> <li>Track List - List of all the tracks as well as providing a way to perform editing functions on those tracks.</li> <li>Timeline - timeline view of tracks and detections, as well as an interface to control the current frame along the video/image-sequence</li> <li>Attributes - Attributes panel used to assign attributes to individual tracks or detections.</li> <li>Context Sidebar - The right sidebar has several different view modes for different types of tasks.<ul> <li>Threshold Controls - Advance thresholding of annotation confidence values per-type.</li> <li>Image Enhancement - Adjust the image threshold range.</li> <li>Group Manager - Controls for creating, managing, and filtering multi-annotation groups.</li> <li>Attributes Details Panel - Attributes panel used to filter or generate graphs of attributes.</li> </ul> </li> </ul>"},{"location":"Command-Line-Tools/","title":"DIVE Command Line Tools","text":"<p>Note</p> <p>This page is not related to the VIAME command line (i.e. <code>kwiver</code>, <code>viame_train_detector</code>)</p> <p>Some of the DIVE data conversion features are exposed through <code>dive</code>.  </p>"},{"location":"Command-Line-Tools/#features","title":"Features","text":"<ul> <li>Convert between various supported formats</li> <li>Verify the integrity of a DIVE Json annotation file.</li> </ul>"},{"location":"Command-Line-Tools/#installation","title":"Installation","text":"<p>Follow the docs in the Debug Utils and Command Line Tools section of <code>server/README.md</code>.</p> <pre><code>git clone https://github.com/Kitware/dive.git\ncd dive/server\npoetry install\n</code></pre>"},{"location":"Command-Line-Tools/#usage","title":"Usage","text":"<pre><code>~$ poetry run dive convert --help\n\n# Usage: dive convert [OPTIONS] COMMAND [ARGS]...\n\n# Options:\n#   --version  Show the version and exit.\n#   --help     Show this message and exit.\n\n# Commands:\n#   coco2dive\n#   dive2viame\n#   kpf2dive\n#   viame2dive\n</code></pre>"},{"location":"DataFormats/","title":"Data Formats","text":"<p>DIVE Desktop and Web support a number of annotation and configuration formats.  The following formats can be uploaded or imported alongside your media and will be automatically parsed.</p> <ul> <li>DIVE Annotation JSON (default annotation format)</li> <li>DIVE Configuration JSON</li> <li>VIAME CSV</li> <li>KPF (KWIVER Packet Format)</li> <li>COCO and KWCOCO (web only)</li> </ul>"},{"location":"DataFormats/#dive-annotation-json","title":"DIVE Annotation JSON","text":"<p>Info</p> <p>The current DIVE schema version is v2.  Version 2 was introduced in DIVE version 1.9.0.  It is backward-compatible with v1.</p> <p>Files are typically named <code>result_{dataset-name}.json</code>.  Their schema is described as follows.</p> <pre><code>/** AnnotationSchema is the schema of the annotation DIVE JSON file */\ninterface AnnotationSchema {\n  tracks: Record&lt;string, TrackData&gt;;\n  groups: Record&lt;string, GroupData&gt;;\n  version: 2;\n}\n\ninterface TrackData {\n  id: AnnotationId;\n  meta: Record&lt;string, unknown&gt;;\n  attributes: Record&lt;string, unknown&gt;;\n  confidencePairs: Array&lt;[string, number]&gt;;\n  begin: number;\n  end: number;\n  features: Array&lt;Feature&gt;;\n}\n\ninterface GroupData {\n  id: AnnotationId;\n  meta: Record&lt;string, unknown&gt;;\n  attributes: Record&lt;string, unknown&gt;;\n  confidencePairs: Array&lt;[string, number]&gt;;\n  begin: number;\n  end: number;\n  /**\n   * members describes the track members of a group,\n   * including sub-intervals that they are participating in the group.\n   */\n  members: Record&lt;AnnotationId, {\n    ranges: [number, number][];\n  }&gt;;\n}\n\ninterface Feature {\n  frame: number;\n  flick?: Readonly&lt;number&gt;;\n  interpolate?: boolean;\n  keyframe?: boolean;\n  bounds?: [number, number, number, number]; // [x1, y1, x2, y2] as (left, top), (bottom, right)\n  geometry?: GeoJSON.FeatureCollection&lt;GeoJSON.Point | GeoJSON.Polygon | GeoJSON.LineString | GeoJSON.Point&gt;;\n  fishLength?: number;\n  attributes?: Record&lt;string, unknown&gt;;\n  head?: [number, number];\n  tail?: [number, number];\n}\n</code></pre> <p>The full source TrackData definition can be found here as a TypeScript interface.</p>"},{"location":"DataFormats/#example-json-file","title":"Example JSON File","text":"<p>This is a relatively simple example, and many optional fields are not included.</p> <pre><code>{\n  \"version\": 2,\n\n  \"tracks\": {\n    // Track 1 is a true multi-frame track\n    \"1\": {\n      \"id\": 1,\n      \"meta\": {},\n      \"attributes\": {},\n      \"confidencePairs\": [[\"fish\", 0.87], [\"rock\", 0.22]],\n      \"features\": [\n        { \"frame\": 0, \"bounds\": [0, 0, 10, 10], \"interpolate\": true },\n        { \"frame\": 3, \"bounds\": [10, 10, 20, 20] },\n      ],\n      \"begin\": 0,\n      \"end\": 2,\n    },\n    // Track 2 is a simple single-frame bounding box detection\n    \"2\": {\n      \"id\": 2,\n      \"meta\": {},\n      \"attributes\": {},\n      \"confidencePairs\": [[\"scallop\", 0.67]],\n      \"features\": [\n        { \"frame\": 3, \"bounds\": [10, 10, 20, 20] },\n      ],\n      \"begin\": 3,\n      \"end\": 3,\n    },\n  },\n\n  \"groups\": {\n    \"1\": {\n      \"id\": 1,\n      \"meta\": {},\n      \"attributes\": {},\n      \"confidencePairs\": [[\"underwater-stuff\", 1.0]],\n      \"members\": {\n        // The fish is a group member on frame 0, 1, and 3.\n        // The scallop is only a group member at frame 3.\n        \"1\": { \"ranges\": [[0, 1], [3, 3]] },\n        \"2\": { \"ranges\": [[3, 3]] },\n      },\n      \"begin\": 0,\n      \"end\": 2,\n    }\n  }\n}\n</code></pre>"},{"location":"DataFormats/#dive-configuration-json","title":"DIVE Configuration JSON","text":"<p>This information provides the specification for an individual dataset.  It consists of the following.</p> <ul> <li>Allowed types (or labels) and their appearances are defined by <code>customTypeStyling</code> and <code>customGroupStyling</code>.</li> <li>Preset confidence filters for those types are defined in <code>confidenceFilters</code></li> <li>Track and Detection attribute specifications are defined in <code>attributes</code></li> </ul> <p>The full DatasetMetaMutable definition can be found here.</p> <pre><code>interface DatasetMetaMutable {\n  version: number;\n  customTypeStyling?: Record&lt;string, CustomStyle&gt;;\n  customGroupStyling?: Record&lt;string, CustomStyle&gt;;\n  confidenceFilters?: Record&lt;string, number&gt;;\n  imageEnhancments?: ImageEnhancements;\n  attributes?: Readonly&lt;Record&lt;string, Attribute&gt;&gt;;\n}\n</code></pre>"},{"location":"DataFormats/#viame-csv","title":"VIAME CSV","text":"<p>Read the VIAME CSV Specification.</p> <p>Warning</p> <p>VIAME CSV is the format that DIVE exports to.  It doesn't support all features of the annotator (like groups) so you may need to use the DIVE Json format.  It's easier to work with.</p>"},{"location":"DataFormats/#kwiver-packet-format-kpf","title":"KWIVER Packet Format (KPF)","text":"<p>DIVE supports MEVA KPF</p> <ul> <li>Read the KPF Specification</li> <li>See example data in meva-data-repo</li> </ul> <p>Info</p> <p>KPF is typically broken into 3 files, but DIVE only supports annotations being loaded as a single file. However, the 3-file breakdown is just convention and KPF can be loaded from a single combined file.</p> <pre><code># Example: create a sinlge KPF yaml annotation file for use in DIVE\ncat 2018-03-07.11-05-07.11-10-07.school.G339.*.yml &gt; combined.yml\n</code></pre>"},{"location":"DataFormats/#coco-and-kwcoco","title":"COCO and KWCOCO","text":"<p>Only supported on web.</p> <ul> <li>Read the COCO Specification</li> <li>Read the KWCOCO Specification</li> </ul>"},{"location":"Deployment-Docker-Compose/","title":"Running with Docker Compose","text":"<p>Start here once you have SSH access and <code>sudo</code> privileges for a server or VM. </p> <p>Note</p> <p>Docker server installation is only supported on Linux distributions</p>"},{"location":"Deployment-Docker-Compose/#container-images","title":"Container Images","text":"<p>A DIVE Web deployment consists of 2 main services.</p> <ul> <li>kitware/viame-web - the web server</li> <li>kitware/viame-worker - the queue worker</li> </ul> <p>In addition, a database (MongoDB) and a queue service (RabbitMQ) are required.</p> <p></p>"},{"location":"Deployment-Docker-Compose/#install-dependencies","title":"Install dependencies","text":"<p>SSH into the target server and install these system dependencies.</p> <p>Tip</p> <p>You can skip this section if you used Ansible to configure your server, as it already installed all necessary dependencies.</p> <ul> <li>Install NVIDIA Driver (version specified in VIAME)<ul> <li><code>sudo ubuntu-drivers install</code> usually works.</li> </ul> </li> <li>Install <code>docker</code> version 19.03+ guide</li> <li>Install <code>docker-compose</code> version 1.28.0+ guide</li> <li>Install nvidia-container-toolkit</li> </ul>"},{"location":"Deployment-Docker-Compose/#basic-deployment","title":"Basic deployment","text":"<p>Clone this repository and configure options in <code>.env</code> .</p> <pre><code># Clone this repository\ngit clone https://github.com/Kitware/dive /opt/dive\n\n# Change to correct directory\ncd /opt/dive\n\n# Initiate the .env file\ncp .env.default .env\n\n# Edit the .env file\n# See configuration options below and inline comments\nnano .env\n\n# Pull pre-built images\ndocker-compose pull\n\n# Bring the services up\n# Make sure to specify docker-compose.yml unless you intend to mount code for development\ndocker-compose -f docker-compose.yml up -d\n</code></pre> <p>VIAME server will be running at http://localhost:8010. You should see a page that looks like this. The default username and password is <code>admin:letmein</code>.</p> <p></p>"},{"location":"Deployment-Docker-Compose/#production-deployment","title":"Production deployment","text":"<p>If you have a server with a public-facing IP address and a domain name that points to it, you should be able to use our production deployment configuration.  This is the way we deploy viame.kitware.com.</p> <ul> <li><code>containrrr/watchtower</code> updates the running containers on a schedule using automated image builds from docker hub (above).</li> <li><code>linuxserver/duplicati</code> is included to schedule nightly backups, but must be manually configured.</li> </ul> <p>You should scale the girder web server up to an appropriate number.  This stack will automatically load-balance across however many instances you bring up.</p> <pre><code># Continuing from above, modify .env again to include the production variables\nnano .env\n\n# pull extra containers\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml pull\n\n# scale the web service up\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale girder=4\n</code></pre>"},{"location":"Deployment-Docker-Compose/#splitting-services","title":"Splitting services","text":"<p>It's possible to split your web server and task runner between multiple nodes.  This may be useful if you want to run DIVE Web without a GPU or if you want to save money by keeping your GPU instance stopped when not in use.  You could also increase parallel task capacity by running task runners on multiple nodes.</p> <ul> <li>Make two cloud VM instances, one with NVIDIA drivers and container toolkit, and one without.  This is still a special case of scenario 1 from the Provisioning Guide</li> <li>Clone the dive repository on both, and set up <code>.env</code> on both with the same configuration.</li> <li>Be sure that <code>WORKER_API_URL</code> and <code>CELERY_BROKER_URL</code> in particular are uncommented and set to the IP or domain name of your web server.  This is how the worker will talk to the web server, so the web server must be network accessible from the worker.</li> </ul> <pre><code>## On the web server\ndocker-compose -f docker-compose.yml up -d girder rabbit\n\n## On the GPU server(s)\ndocker-compose -f docker-compose.yml up -d --no-deps girder_worker_pipelines girder_worker_training girder_worker_default\n</code></pre> <p>In order to run any jobs (video transcoding, pipelines, training, addon upgrades) the GPU server will need to be running.</p>"},{"location":"Deployment-Docker-Compose/#addon-management","title":"Addon management","text":"<p>After initial deployment, DIVE Server will require an addon upgrade in order to download and scan for VIAME addons. Run this by issuing a <code>POST /dive_configuration/upgrade_pipelines</code> request from the swagger UI at <code>http://{server_url}:{server_port}/api/v1</code>.</p> <ul> <li>Whether you <code>force</code> or not, only those pipelines from addons from the exact urls passed will be enabled on the server.</li> <li>An old addon can be disabled by simply omitting its download from the upgrade payload.</li> <li><code>force</code> should be used to force re-download of all URLs in the payload even if their zipfiles have been cached.</li> <li>An upgrade run is always required if the \"common\" pipelines in the base image change.  These are updated for every run, and do not require <code>force</code>.</li> <li>See the job log to verify the exact actions taken by an upgrade job.</li> <li>Optional patches are updated occasionally and you can find the latest urls here.</li> </ul> <p></p>"},{"location":"Deployment-Docker-Compose/#configuration-reference","title":"Configuration Reference","text":""},{"location":"Deployment-Docker-Compose/#server-branding-config","title":"Server branding config","text":"<p>You can configure the brand and messaging that appears in various places in the DIVE Web UI using the config API.</p> <ol> <li>Open the swagger page at /api/v1</li> <li><code>PUT /dive_configuration/brand_data</code> where the body is a JSON object from the template below.  If you do not want to set a value and use the default, omit the key and value from the config body.</li> </ol> <pre><code>{\n  // A JSON Vuetify theme configuration object.\n  // https://vuetify.cn/en/customization/theme/#customizing\n  \"vuetify\": {},\n\n  // A URL to a favicon\n  \"favicon\": \"\",\n\n  // A URL to an image that will be shown as the main logo\n  \"logo\": \"\",\n\n  // Used in several places, including the main toolbar\n  \"name\": \"VIAME\",\n\n  // Message that appears on the login screen\n  \"loginMessage\": \"\",\n\n  // Alert messages are typically used to tell users about maintenance, outages, etc.\n  \"alertMessage\": \"\",\n}\n</code></pre>"},{"location":"Deployment-Docker-Compose/#web-server-config","title":"Web Server config","text":"<p>This image contains both the backend and client.</p> Variable Default Description GIRDER_MONGO_URI <code>mongodb://mongo:27017/girder</code> a mongodb connection string GIRDER_ADMIN_USER <code>admin</code> admin username GIRDER_ADMIN_PASS <code>letmein</code> admin password CELERY_BROKER_URL <code>amqp://guest:guest@default/</code> rabbitmq connection string WORKER_API_URL <code>http://girder:8080/api/v1</code> Address for workers to reach web server <p>There is additional configuration for the RabbitMQ Management plugin. It only matters if you intend to allow individual users to configure private job runners in standalone mode, and can otherwise be ignored.</p> Variable Default Description RABBITMQ_MANAGEMENT_USERNAME <code>guest</code> Management API username RABBITMQ_MANAGEMENT_PASSWORD <code>guest</code> Management API password RABBITMQ_MANAGEMENT_VHOST <code>default</code> Virtual host should match <code>CELERY_BROKER_URL</code> RABBITMQ_MANAGEMENT_URL <code>http://rabbit:15672/</code> Management API Url <p>You can also pass girder configuration and celery configuration.</p>"},{"location":"Deployment-Docker-Compose/#worker-config","title":"Worker config","text":"<p>This image contains a celery worker to run VIAME pipelines and transcoding jobs.</p> <p>Note: Either a broker url or DIVE credentials must be supplied.</p> Variable Default Description WORKER_WATCHING_QUEUES null one of <code>celery</code>, <code>pipelines</code>, <code>training</code>.  Ignored in standalone mode. WORKER_CONCURRENCY <code># of CPU cores</code> max concurrnet jobs. Lower this if you run training WORKER_GPU_UUID null leave empty to use all GPUs.  Specify UUID to use specific device CELERY_BROKER_URL <code>amqp://guest:guest@default/</code> rabbitmq connection string. Ignored in standalone mode. KWIVER_DEFAULT_LOG_LEVEL <code>warn</code> kwiver log level DIVE_USERNAME null Username to start private queue processor. Providing this enables standalone mode. DIVE_PASSWORD null Password for private queue processor. Providing this enables standalone mode. DIVE_API_URL <code>https://viame.kitware.com/api/v1</code> Remote URL to authenticate against <p>You can also pass regular celery configuration variables.</p>"},{"location":"Deployment-Docker-Compose/#running-the-gpu-job-runner-in-standalone-mode","title":"Running the GPU Job Runner in standalone mode","text":"<p>Linux Only.</p> <p>Individual users can run a standalone worker to process private jobs from VIAME Web.</p> <ul> <li>Install VIAME from the github page to <code>/opt/noaa/viame</code>.</li> <li>Activate the install with <code>source setup_viame.sh</code>.</li> <li>Install VIAME pipeline addons by running <code>cd bin &amp;&amp; download_viame_addons.sh</code> from the VIAME install directory.</li> <li>Enable the private user queue for your jobs by visiting the jobs page</li> <li>Run a worker using the docker command below</li> </ul> <p>Note: The <code>--volume</code> mount maps to the host installation.  You may need to change the source from <code>/opt/noaa/viame</code> depending on your install location, but you should not change the destination from <code>/tmp/addons/extracted</code>.</p> <pre><code>docker run --rm --name dive_worker \\\n  --gpus all \\\n  --ipc host \\\n  --volume \"/opt/noaa/viame/:/tmp/addons/extracted:ro\" \\\n  -e \"WORKER_CONCURRENCY=2\" \\\n  -e \"DIVE_USERNAME=CHANGEME\" \\\n  -e \"DIVE_PASSWORD=CHANGEME\" \\\n  -e \"DIVE_API_URL=https://viame.kitware.com/api/v1\" \\\n  kitware/viame-worker:latest\n</code></pre>"},{"location":"Deployment-Overview/","title":"Deployment Overview","text":"<p>The goal of this page is to provide an overview of the ways to run VIAME or VIAME Web in various types of compute environments.</p>"},{"location":"Deployment-Overview/#contents","title":"Contents","text":"<ul> <li>Using our deployment of VIAME Web</li> <li>Running your own instance of VIAME Web</li> <li>Using the VIAME command line and project folders in a cloud environment</li> <li>Hybrid options for using local or cloud compute resources with an existing deployment</li> <li>Hybrid options for integrating data from cloud storage such as GCP Buckets or S3 into an existing deployment</li> </ul>"},{"location":"Deployment-Overview/#our-server-vs-running-your-own","title":"Our server vs running your own","text":"Using our server Running your own Free to use; no maintenance costs You pay hosting and maintenance costs Always up to date Possible to configure automated updates One shared environment for everyone Your organization has full control over access Our team monitors this service for errors and can respond to issues proactively Support requires logs, screenshots, and other error information if applicable Our team can provide guidance on annotation and training because we have direct access to your data Support usually requires example data and annotations Having user data in our environment helps us understand user needs and improve the product Feedback is always appreciated. Limited shared compute resources (2 GPUs) available to process jobs. Can be mitigated by hybrid compute options As much compute as you pay for"},{"location":"Deployment-Overview/#noaa-google-cloud-computing-resources","title":"NOAA Google Cloud Computing resources","text":"<p>For deploying an instance in NOAA Fisheries GCP environment please use the following repository for reference:</p> <p>VIAME-Web NOAA GCP</p>"},{"location":"Deployment-Overview/#using-our-public-server","title":"Using our public server","text":"<p>The easiest option to get started using VIAME is to try our public server.</p>"},{"location":"Deployment-Overview/#running-your-own-instance","title":"Running your own instance","text":"<p>You may wish to run your own deployment of VIAME Web in your lab or a cloud environment.  Deploying VIAME Web is relatively straightforward with <code>docker-compose</code>.</p> Environment Instructions Local server If you already have SSH access to an existing server and <code>sudo</code> permissions, proceed to the docker compose guide. Google\u00a0Cloud Continue to the Provisioning Google Cloud page for <code>Scenario 1</code> AWS / Azure Create a server on your own through the cloud management console, then proceed to the docker compose guide."},{"location":"Deployment-Overview/#viame-cli-with-project-folders","title":"VIAME CLI with project folders","text":"<p>You may not want to use the web annotator and job orchestration at all, and instead run VIAME using the command line in a cloud environment with GPU.</p> Environment Instructions Local server This is a standard VIAME install.  See the VIAME documentation install instructions. Google\u00a0Cloud Continue to the Provisioning Google Cloud page for <code>Scenario 2</code> AWS / Azure Create a server through the cloud management console, then proceed to the VIAME documentation install instructions."},{"location":"Deployment-Overview/#hybrid-options-for-compute","title":"Hybrid options for compute","text":"<p>Instead of running the whole web stack, it's possible to deploy a worker by itself to process compute-intensive jobs.  This is referred to in the docs as standalone mode. For example, you could:</p> <ul> <li>Upload and annotate at viame.kitware.com, but run your own private worker on a lab workstation</li> <li>Deploy your own web server to a local lab workstation, but process your jobs in an ephemeral Google Cloud VM.</li> </ul> <p>How it works</p> <ul> <li>You must toggle your private queue</li> <li>When you launch jobs (like transcoding, pipelines, or training), they go into a special queue just for your user account.</li> <li>You are responsible for running a worker.  Your worker is a Celery process that will connect to our public RabbitMQ server.</li> <li>Jobs submitted through the interface at viame.kitware.com will run on your compute resources.  This involves automatically downloading the video or images and annotation files, running a kwiver pipeline, and uploading the results.</li> </ul> <p>To set up a private worker, continue to the Provisioning Google Cloud page for <code>Scenario 3</code>.</p>"},{"location":"Deployment-Overview/#hybrid-options-for-storage","title":"Hybrid options for storage","text":"<p>Any instance of VIAME Web, including our public server, can connect to S3-compatible storage.  This means your lab or group could make your existing data available at viame.kitware.com, either privately or publicly.</p> Storage Product Support level Google Cloud Buckets Use as backing storage, import existing data, monitor for changes and automatically discover new uploads AWS S3 Use as backing storage, import existing data MinIO Use as backing storage, import existing data Azure Blob Storage Limited import support using MinIO Azure Gateway"},{"location":"Deployment-Overview/#get-help","title":"Get Help","text":"<p>Contact us for support with any of these topics.</p>"},{"location":"Deployment-Provision/","title":"Cloud Deployment Guide","text":"<p>Note</p> <p>Be sure to read the Deployment Overview first.</p> <ul> <li>Scenario 1: Deploy your own instance of VIAME Web to GCP Compute Engine.</li> <li>Scenario 2: Run VIAME pipelines on a GCP Compute Engine VM from the command line.</li> <li>Scenario 3: Run a Private GPU worker in GCP to process jobs from any VIAME Web instance including viame.kitware.com (standalone mode)</li> </ul> <p>The terraform section is the same for all scenarios.  The Ansible section will have differences.</p>"},{"location":"Deployment-Provision/#before-you-begin","title":"Before you begin","text":"<p>You'll need a GCP Virtual Machine (VM) with the features listed below.  This section will guide you through creating one and deploying VIAME using Terraform and Ansible.</p> <ul> <li>Terraform automates the process of creating and destroying cloud resources such as VMs.</li> <li>Ansible automates configuration, such as software installation, on newly created machines.</li> </ul> <p>Together, these tools allow you to quickly create a reproducible environment.  If you do not want to use these tools, you can create your own VM manually through the management console and skip to the docker documentation instead.</p> Feature Recommended value Operating system Ubuntu 20.04 Instance Type <code>n1-standard-4</code> or larger GPU Type <code>nvidia-tesla-t4</code>, <code>nvidia-tesla-p4</code>, or similar Disk Type SSD, 128GB or more depending on your needs"},{"location":"Deployment-Provision/#install-dependencies","title":"Install dependencies","text":"<p>To run the provisioning tools below, you need the following installed on your own workstation.</p> <p>Note</p> <p>Google Cloud worker provisioning can only be done from an Ubuntu Linux 18.04+ host.  Ansible and terraform should work on Windows Subsystem for Linux (WSL) if you only have a windows host.  You could also use a cheap CPU-only cloud instance to run these tools.</p> <ul> <li>Install Google Cloud SDK</li> <li>Install Terraform</li> <li>Install Ansible</li> <li>Find your google cloud project id.  It looks like <code>project-name-123456</code>.</li> </ul> <p>Tip</p> <p>Google Cloud imposes GPU Quotas.  You may need to request a quota increase.  Anecdotally, request increases of 1 unit are approved automatically, but more are rejected.</p>"},{"location":"Deployment-Provision/#creating-a-vm-with-terraform","title":"Creating a VM with Terraform","text":"<pre><code># Clone the dive repo\ngit clone https://github.com/Kitware/dive.git\ncd dive/devops\n\n# Generate a new ssh key\nssh-keygen -t ed25519 -f ~/.ssh/gcloud_key\n</code></pre>"},{"location":"Deployment-Provision/#run-terraform","title":"Run Terraform","text":"<p>Warning</p> <p>GPU-accelerated VMs are significantly more expensive than typical VMs.  Make sure you are familiar with the cost of the machine and GPU you choose.  See main.tf for default values.</p> <p>See <code>devops/main.tf</code> for a complete list of variables.  The default <code>machine_type</code>, <code>gpu_type</code>, and <code>gpu_count</code> can be overridden.</p> <pre><code># Authenticate with google cloud\ngcloud auth application-default login\n\n# Verify your GPU Quota\n# https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus\n# REGION might change.\ngcloud compute regions describe us-central1\n\n# Run plan, providing any variables you choose\nterraform plan \\\n    -var \"project_name=&lt;GCloud-Project-Id&gt;\" \\\n    -var \"gpu_count=1\" \\\n    -out create.plan\n\n# Run apply.  It may take several minutes\nterraform apply create.plan\n</code></pre>"},{"location":"Deployment-Provision/#destroy-the-stack","title":"Destroy the stack","text":"<p>Later, when you are done with the server and have backed up your data, use terraform to destroy your resources.</p> <pre><code>terraform destroy -var \"project_name=&lt;GCloud-Project-Id&gt;\"\n</code></pre>"},{"location":"Deployment-Provision/#configure-with-ansible","title":"Configure with Ansible","text":"<p>This step will prepare the new host to run a VIAME worker by installing nvidia drivers, docker, and downloading VIAME and all optional addons.</p> <p>Warning</p> <p>The playbook may take 30 minutes or more to run because it must install nvidia drivers and download several GB of software packages.</p>"},{"location":"Deployment-Provision/#ansible-extra-vars","title":"Ansible Extra Vars","text":"<p>These are all the variables that can be provided with <code>--extra-vars</code>.</p> Variable Default Description run_server <code>no</code> Set <code>run_server=yes</code> for scenario 1 (Web Instance) (Fastest option) run_viame_cli <code>no</code> Set <code>run_viame_cli=yes</code> for scenario 2 (VIAME CLI) run_worker_container <code>no</code> Set <code>run_worker_container=yes</code> for scenario 3 (Standalone Worker) viame_bundle_url latest bundle url Optional for scenario 2 &amp; 3.  Change to install a different version of VIAME.  This should be a link to the latest Ubuntu Desktop (18/20) binaries from viame.kitware.com (Mirror 1) DIVE_USERNAME null Required for scenario 3. Username to start private queue processor DIVE_PASSWORD null Required for scenario 3. Password for private queue processor WORKER_CONCURRENCY <code>2</code> Optional for scenario 3. Max concurrnet jobs. Change this to 1 if you run training DIVE_API_URL <code>https://viame.kitware.com/api/v1</code> Optional for scenario 3. Remote URL to authenticate against. KWIVER_DEFAULT_LOG_LEVEL <code>warn</code> Optional for scenario 3. kwiver log level"},{"location":"Deployment-Provision/#run-ansible","title":"Run Ansible","text":"<p>The examples below assumes the <code>inventory</code> file was created by Terraform above.</p> <pre><code># install galaxy plugins\nansible-galaxy install -r ansible/requirements.yml\n\n# Choose only 1 of the scenarios below\n\n# Scenario 1 (Web Instance) Example\nansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_server=yes\"\n\n# Scenario 2 (VIAME CLI) Example\nansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_viame_cli=yes\"\n\n# Scenario 3 (Standalone Worker) Example\nansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_worker_container=yes DIVE_USERNAME=username DIVE_PASSWORD=changeme\"\n</code></pre> <p>Once provisioning is complete, jobs should begin processing from the job queue.  You can check viame.kitware.com/#/jobs to see queue progress and logs.</p> <p>Tip</p> <p>This Ansible playbook is runnable from any Ubuntu 18.04+ host to any Ubuntu 18.04+ target.  To run it locally, use the <code>inventory.local</code> file instead.  If you already have nvidia or docker installed, you can comment out these lines in the playbook.</p> <p>If you run locally you'll need to restart the machine and run the playbook a second time.  The playbook will do this automatically for remote provisioning, but cannot restart if you're provisioning localhost.</p> <pre><code>ansible-playbook --ask-become-pass -i inventory ansible/playbook.yml --extra-vars \"&lt;see above&gt;\"\n</code></pre> <p>Tip</p> <p>You may need to run through the docker post-install guide if you have permissions errors when trying to run <code>docker</code>.</p>"},{"location":"Deployment-Provision/#check-that-it-worked","title":"Check that it worked","text":"<pre><code># Log in with the ip address from the inventory file (or google cloud dashboard)\nssh -i ~/.ssh/gcloud_key viame@ip-address\n\n# Test nvidia docker installation\ndocker run --gpus=all --rm nvidia/cuda:11.0-base nvidia-smi\n\n# Test regular nvidia runtime\nnvidia-smi\n\n# For Scenario 2 and 3, check KWIVER installation\ncd /opt/noaa/viame\nsource setup_viame.sh\nkwiver\n\n# For Scenario 3, you can check to see if the worker is started\n# You should see \"celery@identifier ready.\" in the logs\nsudo docker logs -f worker\n</code></pre> <p>You can enable your private queue on the jobs page and begin running jobs.</p>"},{"location":"Deployment-Provision/#next-steps","title":"Next Steps","text":"<ul> <li>Scenario 1: Proceed to Docker Compose Deployment.</li> <li>Scenario 2: Setup is complete.  Proceed to the VIAME Documentation.</li> <li>Scenario 3: Setup is complete.  Make sure your private queue is enabled.</li> </ul>"},{"location":"Deployment-Provision/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Ansible provisioning is idempotent.  If it fails, run it again once or twice.</li> <li>You may need to change the global <code>GPUS_ALL_REGIONS</code> quota in IAM -&gt; Quotas</li> <li>Nvidia drivers may not install correctly the first time.  Try installing manually using <code>ubuntu-drivers</code></li> </ul>"},{"location":"Deployment-Storage/","title":"Cloud Storage Integration","text":"<p>This page is intended for storage administrators who would like to make their existing data available through VIAME Web.</p> <p>Tip</p> <p>This guide assumes you are working with viame.kitware.com.  If you are using a different deployment, be sure to change the appropriate fields.</p> <p>Tip</p> <p>Regarding data transfer costs, if you choose to keep both your data storage and job runners in Google Cloud (or AWS), you will avoid paying a data egress fee for transferring data between storage and the processing node.</p>"},{"location":"Deployment-Storage/#google-cloud-storage-mirroring","title":"Google Cloud Storage Mirroring","text":"<p>DIVE Web can mirror your data from Google Cloud storage buckets such that your team fully controls upload and data organization, but is able to view, annotate, and run analysis within VIAME Web.</p>"},{"location":"Deployment-Storage/#creating-access-credentials","title":"Creating access credentials","text":"<ol> <li>Create a new service account with read-only access to the bucket(s) and prefixes that you want to map.</li> <li>In storage settings, in the interoperability tab<ol> <li>create an access key (Service account HMAC) for your read-only service account.</li> <li>set the current project as the default project for interoperability access</li> <li>take note of your <code>Access Key</code>, <code>Secret Key</code>, <code>Storage URI</code>, and <code>Bucket Name</code>.</li> </ol> </li> </ol>"},{"location":"Deployment-Storage/#setting-up-cors","title":"Setting up CORS","text":"<p>You'll also need to configure CORS headers for any buckets where media will be served.</p> <ul> <li>Save the snippet below as <code>bucket-cors-config.json</code>.</li> <li><code>\"origin\"</code> should be whatever you type into your browser to get to the web application.</li> </ul> <pre><code>  [\n    {\n      \"origin\": [\"https://viame.kitware.com\"],\n      \"method\": [\"GET\", \"PUT\", \"POST\", \"DELETE\"],\n      \"responseHeader\": [\"Content-Type\"],\n      \"maxAgeSeconds\": 3600\n    }\n  ]\n</code></pre> <p>Then use <code>gsutils</code> to configure each bucket.</p> <pre><code>gsutil cors set bucket-cors-config.json gs://BUCKET_NAME\n</code></pre>"},{"location":"Deployment-Storage/#choose-a-mount-point","title":"Choose a mount point","text":"<p>Choose a folder as a mount-point inside DIVE Web.  This folder should ideally be dedicated to mapping from your GCS buckets.</p> <p>We recommend creating a <code>Google Cloud Storage</code> folder with sub-folders named for each bucket you mount.  You can do this using the <code>New Folder</code> button in DIVE Web's File Browser.  You can get the folder ID from your browser's URL bar.</p>"},{"location":"Deployment-Storage/#send-us-the-details","title":"Send us the details","text":"<p>If you want to use your bucket with viame.kitware.com, send us an email with the following details to <code>viame-web@kitware.com</code>.</p> <pre><code>subject: Add a google cloud storage bucket mount\n\nBucket name:\nService provider: Google cloud\nAccess Key: \nSecret Key:\nMount point folder:\nPrefix (if applicable):\n</code></pre>"},{"location":"Deployment-Storage/#s3-and-minio-mirroring","title":"S3 and MinIO Mirroring","text":"<p>If you have data in S3 or MinIO, you can mirror it in DIVE for annotation.</p> <ul> <li>Data is expected to be either videos or images organized into folders</li> <li>You should not make changes to folder contents once a folder has been mirrored into DIVE.  Adding or removing images in a particular folder may cause annotation alignment issues.</li> <li>Adding entire new folders is supported, and will require a re-index of your S3 bucket.</li> </ul>"},{"location":"Deployment-Storage/#s3minio-and-annotation-importing","title":"S3/MinIO and Annotation Importing","text":"<p>During the importing process annotations that are associated with image-sequences or video files can be automatically imported</p> <ul> <li>Video - For video files the annotation file (CSV or JSON) needs to have the same name as the video with a changed extension. I.E.  video.mp4 will have either video.csv or video.json.  This will automatically import those annotations when the S3/GCP indexing/importing is done</li> <li>Image Sequence - Image-Sequences should already be in their own folder.  The annotation file (CSV or JSON) needs to just be in the same file.  It shouldn't matter what the name of the file is during importing.</li> </ul>"},{"location":"Deployment-Storage/#pubsub-notifications","title":"Pub/Sub notifications","text":"<p>Creating pub/sub notifications is optional, but will keep your mount point up-to-date automatically with new data added to the bucket.  In order to make use of this feature, your DIVE server must have a public static IP address or domain name.</p> <ol> <li>Create a bucket notification configuration</li> <li>Create a topic subscription</li> <li>Set a push delivery method for the subscription<ol> <li>The URL for delivery should be <code>https://viame.kitware.com/api/v1/bucket_notifications/gcs</code></li> </ol> </li> </ol> <p>Our server will process events from this subscription to keep your data current.</p>"},{"location":"Deployment-Storage/#mirroring-setup","title":"Mirroring setup","text":"<p>If you have your own dive deployment, you can create a bucket mirror yourself through the Girder admin console.</p> <ol> <li>Open <code>/girder#assetstores</code> in your browser.<ol> <li>Choose  Create new Amazon S3 Assetstore</li> <li>Enter a name and all the details you collected above.</li> <li>For region, enter the AWS or GCS region you're using, like <code>us-east-1</code>.</li> <li>For service, enter the service URI if you're using an S3 provider other than AWS (such as MinIO or GCS).</li> <li>Mark as Read only.</li> </ol> </li> <li>Now import your data.  Choose the green Begin Import button on the new assetstore.<ol> <li>Leave Import path blank unless you only want to import part of a bucket.</li> <li>For Destination type, use the folder ID you chose as the mount point above.</li> </ol> </li> </ol> <p>The import may take several minutes.  You should begin to see datasets appear inside the mount point folder you chose.</p>"},{"location":"Dive-Desktop/","title":"DIVE Desktop","text":"<p>DIVE is available as an electron based desktop application with VIAME integration. It has most of the same UI and features web.  You may want to use desktop if...</p> <ul> <li>You want to make use of GPUs on your own workstation</li> <li>You need to use DIVE without network access</li> <li>You have large quantities of data on disk impractical for uploading to a server.</li> </ul> <p>DIVE Desktop is fully supported on Windows and Linux. MacOS users can use it as an annotator, but without NVIDIA Driver support, the machine learning features from VIAME are unavailable.</p> <p></p>"},{"location":"Dive-Desktop/#installation","title":"Installation","text":"<p> Download the latest DIVE Desktop from GitHub</p> <p>Choose an asset from the list matching your operating system:</p> OS Extension Description Windows .exe Portable executable (recommended) Windows .msi Installer file MacOS .dmg MacOS DiskImage (Intel only, M1 not supported) Linux .AppImage Portable executable for all Linux platforms (recommended) Linux .snap Ubuntu SnapCraft package"},{"location":"Dive-Desktop/#full-viame-desktop-installation","title":"Full VIAME Desktop Installation","text":"<p>This is the installation guide for DIVE.  If you want the full VIAME toolkit, you can get it from github.com/viame/viame.  The full toolkit installation includes DIVE.</p>"},{"location":"Dive-Desktop/#supported-dataset-types","title":"Supported Dataset Types","text":"<p>DIVE Desktop supports single- and multi-camera datasets.</p> <ul> <li>Single Camera Dataset is the most common option.  Single camera datasets are supported by the majority of VIAME pipeline and training configurations.</li> <li>Stereo Datasets are for datasets collected from a camera rig with a left and right camera.  These datasets can be used with certain specialty VIAME pipelines. Their physical relationship may be described by a camera transform <code>.npz</code> file (numpy transformation matrix).</li> <li>Multi-Cam Datasets are for more generic multi-camera rig setups.  They may have overlapping fields of view.</li> </ul>"},{"location":"Dive-Desktop/#importing-datasets","title":"Importing Datasets","text":"<p>Click either Open Image Sequence  or Open Video  to begin a single camera default import.  Click the  dropdown button to show additional import options.</p> <ul> <li> From File is the default option for videos. It will open a file picker and allow you to choose a single video file.</li> <li> Directory is the default option for image sequences. It will prompt you to choose an entire folder of images to import as a dataset.<ul> <li>You can use globbing patterns to filter the contents of an image directory during import. Click  Show advanced options to reveal the glob input.</li> </ul> </li> <li> Image List will prompt you to choose a <code>.txt</code> file that contains an image name or full path on each line.</li> <li> Stereo will prompt you to choose 2 videos or 2 image sequences and a calibration file.</li> <li> Multi-Cam will prompt you to describe the multi-cam configuration by naming several cameras and picking the source media for each.</li> </ul> <p>The import routine will look for <code>.csv</code> and <code>.json</code> files in the same directory as the source media, and you will be prompted to manually select an annotation file and a configuration file.  Neither is required.</p>"},{"location":"Dive-Desktop/#video-transcoding","title":"Video Transcoding","text":"<p>DIVE Desktop is an Electron application built on web technologies.  Certain video codecs require automatic transcoding to be usable.  Video will be transcoded unless all the following conditions are met.</p> <ul> <li><code>codec</code> = <code>h264</code></li> <li><code>sample_aspect_ratio (SAR)</code> = <code>1:1</code></li> </ul> <p>Transcoding is done with ffmpeg, which comes bundled with the DIVE Desktop executable.</p>"},{"location":"Dive-Desktop/#running-training","title":"Running Training","text":"<ol> <li>Click on  Training to open the training tab.</li> <li>Add one or more datasets to the staging area by clicking .</li> <li>Choose an appropriate training config file and any training parameters.  These are documented on the training configuration page.</li> <li>Click Train on (N) Datasets.  Note that depending on what configuration and datasets you chose, training could take hours or days.</li> </ol>"},{"location":"Dive-Desktop/#desktop-settings","title":"Desktop Settings","text":"<p>DIVE Desktop requires a local installation of the VIAME toolkit to run ML pipelines and training.</p> <ul> <li>VIAME Install Path is set automatically if you use the <code>launch_dive_interface.[bat|sh]</code> script from a VIAME install.  Otherwise, you may need to change this yourself.<ul> <li>Use Choose  to choose the base installation path, then click  Save.</li> </ul> </li> <li>Project Data Storage Path defaults to a subfolder in your user workspace and should generally not be changed.</li> <li>Read only mode disables the ability to save when using the annotator.</li> <li>Synchronize Library - The  Synchronize Library with Project Data button is useful if data in the Project Data Storage Path gets out of sync with what appears in the  Library list.</li> </ul>"},{"location":"Dive-Desktop/#data-storage-path","title":"Data Storage Path","text":"<p>The data storage path is not at all related to \"project folders\" in VIAME.  It's just a place for DIVE Desktop to keep and structure all the data it needs to run.</p> <p>A typical data storage directory has 3 subfolders:</p> <ul> <li><code>DIVE_Jobs</code> - Each job run has a working directory, kept here.</li> <li><code>DIVE_Projects</code> - Each dataset you import into desktop has metadata and annotation data (with revision history) kept here.</li> <li><code>DIVE_Pipelines</code> - Training runs produce models that get copied into here.</li> </ul> <p>Here's an example of structure you might find in the storage path.</p> <pre><code>VIAME_DATA\n\u251c\u2500\u2500 DIVE_Jobs\n\u2502  \u251c\u2500\u2500 Scallop_1_scallop and flatfish_06-01-2021_11-02-11.585\n\u2502  \u2502  \u251c\u2500\u2500 detector_output.csv\n\u2502  \u2502  \u251c\u2500\u2500 dive_job_manifest.json\n\u2502  \u2502  \u251c\u2500\u2500 image-manifest.txt\n\u2502  \u2502  \u2514\u2500\u2500 runlog.txt\n\u2502  \u2514\u2500\u2500 Scallop_2_scallop netharn_06-01-2021_11-02-19.432\n\u2502     \u251c\u2500\u2500 detector_output.csv\n\u2502     \u251c\u2500\u2500 dive_job_manifest.json\n\u2502     \u251c\u2500\u2500 image-manifest.txt\n\u2502     \u2514\u2500\u2500 runlog.txt\n\u251c\u2500\u2500 DIVE_Pipelines\n\u2502  \u251c\u2500\u2500 My Fish SVM Demo\n\u2502  \u2502  \u251c\u2500\u2500 detector.pipe\n\u2502  \u2502  \u2514\u2500\u2500 fish.svm\n\u2502  \u2514\u2500\u2500 Quadcam_Fish_Detector_SVM\n\u2502     \u251c\u2500\u2500 detector.pipe\n\u2502     \u2514\u2500\u2500 fish.svm\n\u2514\u2500\u2500 DIVE_Projects\n   \u251c\u2500\u2500 fish_training_data_c_jp7hq88vfv\n   \u2502  \u251c\u2500\u2500 auxiliary\n   \u2502  \u2502  \u2514\u2500\u2500 result_06-01-2021_10-55-38.627.json\n   \u2502  \u251c\u2500\u2500 meta.json\n   \u2502  \u2514\u2500\u2500 result_06-01-2021_04-53-38.050.json\n   \u2514\u2500\u2500 scallop_2_jrgdq760gu\n      \u251c\u2500\u2500 auxiliary\n      \u2502  \u2514\u2500\u2500 result_06-01-2021_10-54-56.034.json\n      \u251c\u2500\u2500 meta.json\n      \u2514\u2500\u2500 result_06-01-2021_11-02-35.857.json\n</code></pre>"},{"location":"Dive-Desktop/#configuration-with-env","title":"Configuration with env","text":"<p>DIVE Desktop looks for the these environment variables on launch.</p> Name Default Description DIVE_VIAME_INSTALL_PATH /opt/noaa/viame (Linux/macOS) C:\\Program Files\\VIAME (Windows) Overrides the location of the VIAME installation.  Users may not change this value in the settings pane if provided. DIVE_READONLY_MODE None Overrides read only mode to true or false.  Users may still change this value in the settings pane if provided."},{"location":"Dive-Desktop/#importexport-of-models","title":"Import/Export of Models","text":"<p>Trained models are kept in <code>${Project Data Storage Path}/DIVE_Pipelines</code> as described above.  Each model file consists of exactly 1 pipe file and some number of other model files.</p> <ul> <li>The pipe file can be one of <code>detector.pipe</code>, <code>tracker.pipe</code>, or <code>generate.pipe</code>.</li> <li>Other files can be <code>.zip</code>, <code>.svm</code>, <code>.lbl</code>, or <code>.cfg</code>.</li> </ul> <p>You can use externally trained models in DIVE by creating a folder containing these files.  The name of the configuration or pipeline in dive will be the folder name you create.</p>"},{"location":"Dive-Desktop/#troubleshooting","title":"Troubleshooting","text":"<p>I imported some data, but I don't see my annotations</p> <p>See Importing images and video above.  You most likely need to specify your annotation file in the import dialog.</p> <p>Some VIAME canned pipelines are missing, or there are no training configuration files.</p> <p>You may need to install VIAME Toolkit, or correct your VIAME Install Base Path setting.</p> <p>If you don't see some pipelines you expect, you may not have installed the addons (also called Optional Patches) yet.  Download and install these based on the VIAME installation docs.  </p> <p>Advanced troubleshooting</p> <p>If you experience problems or have questions about DIVE Desktop, contact us and include the content from the settings page such as <code>Build Version</code> as well as your currently installed VIAME version.</p> <p>It's also helpful to look in the debug console.  Press Ctrl+Shift+I to launch the Dev Tools and look under the console tab.  Errors and warnings will appear in red and yellow.  You can right-click in the console area and click \"Save As\" to save the log file and open a support ticket</p> <p></p>"},{"location":"FAQ/","title":"Frequently Asked Questions","text":""},{"location":"FAQ/#how-do-i-find-existing-data-to-use","title":"How do I find existing data to use?","text":"<p>The Training Data Collection is organized roughly by domain and collection method.</p>"},{"location":"FAQ/#how-do-i-share-data-with-others","title":"How do I share data with others?","text":"<p>This use case is covered on the sharing page.</p> <p>If you want to publish your data so that other groups can use it, please contact us.</p>"},{"location":"FAQ/#how-do-i-run-analysis-workflows-on-my-data","title":"How do I run analysis workflows on my data?","text":"<p>In DIVE, these are called pipelines.  You'll need to see what sorts of analysis workflows are currently available on the pipeline page.</p> <p>These sorts of AI workflows are the final goal for most users.  They allow the user to quickly perform quantitative analysis to answer questions like how many individuals of each type appear on each image or video frame?</p> <p>If no suitable existing analysis exists for your use case or you aren't sure how to proceed, you're welcome to contact our team and ask for help.</p>"},{"location":"FAQ/#how-do-i-create-new-models","title":"How do I create new models?","text":"<p>You want to perform analysis (detection, tracking, measurement, etc) on object types not yet covered by the community data and pre-trained analysis pipelines available. This will involve training new models based on ground-truth annotations.</p> <p>Training configurations are listed on the pipeline page.</p>"},{"location":"FAQ/#how-can-i-load-data-incrementally","title":"How can I load data incrementally?","text":"<p>If you have data in lots of places or it arrives at different times, it's probably best to break these batches or groups into individual datasets and annotate each individually.  Using the checkboxes in web, you can use multiple datasets to generate a trained model.</p> <p>Breaking large amounts of data up into manageable groups is generally a good idea.</p>"},{"location":"FAQ/#is-it-possible-to-get-email-notifications-when-a-job-is-done","title":"Is it possible to get email notifications when a job is done?","text":"<p>DIVE Web doesn't currently support notifications through email for job status changes.  You can check your job status through the web interface.</p>"},{"location":"FAQ/#do-users-need-to-transcode-their-own-data","title":"Do users need to transcode their own data?","text":"<p>No. VIAME Web and DIVE Desktop perform automatic transcoding if it is necessary.</p>"},{"location":"FAQ/#how-does-video-frame-alignment-work","title":"How does video frame alignment work?","text":"<p>When you annotate a video in DIVE, the true video is played in the browser using a native HTML5 video player.</p> <p>Web browsers report and control time in floating point seconds rather than using frame numbers, but annotations are created using frame numbers as their time indicators, so it's important to make sure these line up.</p> <p>Most of the time, videos are downsampled for annotation, meaning that the true video framerate (30hz, for example) is annotated at a lower rate, such as 5hz or 10hz.  Kwiver (the computer vision tool behind VIAME) uses a downsampling approach that sorts actual frames into downsampled buckets based on the start time of the frame.</p> <p>An implementation of this approach is described here.</p> <pre><code>def get_frame_from_timestamp(timestamp, true_fps, downsample_fps):\n  downsampled_frame = timestamp * downsample_fps\n  real_frame = \n\n  if downsample_fps &gt;= true_fps:\n    # This is a true downsample\n    next_true_frame_boundary = Math.ceil(timestamp * true_fps)\n    return Math.floor(next_true_frame_boundary / downsample_fps)\n\n  raise Exception('Real video framerate must be GTE downsample rate')\n</code></pre> <p>There are caveats with this approach.</p> <ul> <li>It does not handle padding properly.  If a video begins or ends with padding, you may see a black screen in DIVE, but kwiver will wait for the first true frame to use as the representative for the bucket.</li> <li>It does not handle variable width frames properly.  If a video has variable width frames, the assumptions about the locations of true frame boundaries do not hold and kwiver training may have alignment issues.</li> </ul>"},{"location":"Mouse-Keyboard-Shortcuts/","title":"Mouse and Keyboard Shortcuts","text":""},{"location":"Mouse-Keyboard-Shortcuts/#general-interactions","title":"General Interactions","text":"control description Left Click select track/detection Right Click toggle edit mode Middle Click pan camera Scroll Wheel zoom Mouse Drag pan Shift + mouse drag select area to zoom Up select previous track in list Down select next track in list Esc unselect, exit edit mode A toggle attribute / merge pane"},{"location":"Mouse-Keyboard-Shortcuts/#playback","title":"Playback","text":"control description Left or D previous frame Right or F next frame Space play/pause"},{"location":"Mouse-Keyboard-Shortcuts/#editing","title":"Editing","text":"<p>Most editing controls are available when a track or detection is selected.</p> control description Del delete entire track or detection N create new track or detection Home go to first frame of selected track End go to the last frame of selected track 1 Enter bounding-box edit mode on selection 2 Enter polygon edit mode on selection 3 Enter head/tail/line edit mode on selection H or G while in line mode, place head point next T or Y while in line mode, place tail point next Esc unselect, exit edit mode, exit merge mode K toggle keyframe for the current frame and selected track I toggle interpolation for the current range of the selected track M enter merge mode for on selection Shift+M commit (finalize) merge for selected tracks. G create new group including the selected track Shift+Enter focus class select/text box on selected track in track list.  Press Down to open all options.  Pres Enter twice to accept an option.  Press Esc to unfocus."},{"location":"Mouse-Keyboard-Shortcuts/#adding-new-shortcuts","title":"Adding new shortcuts","text":"<p>If you think a new shortcut or hotkey would be useful, please send us feedback.</p>"},{"location":"Multicamera-data/","title":"Multicamera and Stereo Data (Dive Desktop Beta)","text":"<p>A beta release of the Desktop Only software supports both multicamera and stereo datasets for viewing and editing.</p>"},{"location":"Multicamera-data/#loading-multicamera-data","title":"Loading MultiCamera data","text":"<p>Dive Desktop has documenation under \"Supported Dataset Types\" which describes how to import multiCamera or stereoscopic data.  Reminder that stereoscopic data requies a calibration file where multicamera data doesn't.</p>"},{"location":"Multicamera-data/#datatrack-organization","title":"Data/Track Organization","text":"<p>Data is loaded amongst multiple folders to creae a mutlicamera dataset.  In these cases trackIds will be linked if they are the same across the cameras.  Selection of a trackId that exists across multiple cameras will be linked together in the Track List.</p> <p>Example  If Camera 1 and Camera 2 both have annotation files with TrackId 1 they will be automatically be linked together and selecting one will select them in both cameras.&gt; </p>"},{"location":"Multicamera-data/#camera-selection","title":"Camera Selection","text":"<p>Editing and interacting with a camera requires that you select the camera first.  There is a dropdown in the upper right of the screen which contains the name of the currently selected camera.  Also the currently selected camera will contain a dashed light blue outline.  Left or Right clicking within a camera will cause that camera to be selected.</p>"},{"location":"Multicamera-data/#creating-tracksdetections","title":"Creating Tracks/Detections","text":"<p>Track creation for a single camera works much in the same way it does for single camera datasets.  Using the New Track button or N key to create a new track and draw.  To quickly create a track on another camera and have it link to the current cameras can be done using the \"MultiCamera Tools\" or by selecting the the desired base track and right clicking on the new camera to add the track.  This will put the annotation tool into creation mode for the current TrackId on a new camera.  Alternatively the MultiCamera Tools panel can simplify this be clicking on the Edit button.</p>"},{"location":"Multicamera-data/#multicamera-tools","title":"MultiCamera Tools","text":"<p>Next to the dropdown for the camera selection is a camera settings icon.  Clicking on that will open the context menu.  Within this menu is a dropdown for selecting MultiCamTools.  These tools provide a quick view of the selected track across all cameras. When a track is selected it will easily show the existing detections and tracks across multiple cameras.</p> <p></p> <ul> <li>Editing - Clicking on the  or  edit button for any camera will select that camera and edit an existing track or allow for the creation of a new track which is linked to existing tracks.</li> <li>Deleting (Detection/Track) -   deleting the detection will leave the track for the camera (if it exists on multiple frames) or will remove only the detection for the current frame.  If it is the only detection left on that camera a prompt will ask if you want to delete the track.  If you delete the track it will remove all detections associated with that TrackId across all of the frames.</li> <li>Unlinking -  Will split off the track for the camera into a new trackId</li> <li>Linking -  Will select the new camera and place it into Linking Mode.  This requires selecting a track that is only that camera to link to the currently selected track.  Attempting to link a track that exists across multiple cameras will prompt to split off the track before linking.  To exit linking mode use the Esc Key</li> </ul>"},{"location":"Multicamera-data/#importexport","title":"Import/Export","text":"<p>Importing and exporting of data works similarily to a single dataset except that it will occur on the current selected camera.  Selecting \"Starboard\" camera and clicking export will only export the annotations for the \"Starboard\" camera.  Similarily importing annotations will only occur on the selected camera as well.</p>"},{"location":"Multicamera-data/#running-pipelines","title":"Running Pipelines","text":""},{"location":"Multicamera-data/#single-camera-pipelines","title":"Single Camera Pipelines","text":"<p>Single camera pipelines can be used by selecting the camera and then running the pipeline from the pipeline menu.</p> <p>Note it is suggested that single camera pipelines only be run on empty datasets that don't have annotations already.  When the pipeline finishes it will create tracks with TrackIds that may conflict with the other cameras.  So it is reccomended that all tracks be removed before running single camera pipelines.</p>"},{"location":"Multicamera-data/#multicamerastereo-pipelines","title":"MultiCamera/Stereo Pipelines","text":"<p>There are specific pipelines that can be used on multi-camera or stereo datasets.  These pipelines are related to the type of dataset (mutlicamera vs stereo) and the number of cameras that exist.</p> Pipeline Category Type Cameras Description <pre>gmm</pre> measurment stereoscopic 2 Stereo pipeline used to compute fish length measurement <pre>X-cam</pre> X-cam multicamera 2 or 3 Multiple pipelines that can act on either 2 or 3 camera datasets."},{"location":"Pipeline-Documentation/","title":"Pipelines and Training","text":"<p>Both web and desktop versions are capable of running canned pipelines and model training on your ground truth data.  This document is to help you decide which pipeline to run.</p>"},{"location":"Pipeline-Documentation/#help-me-choose","title":"Help me choose","text":"<p>Contact our team if you need help choosing the right data analysis strategy.  Please upload some sample data to viame.kitware.com to allow us to better assist you.</p>"},{"location":"Pipeline-Documentation/#detection","title":"Detection","text":"<p>Best for a series of images that have no temporal relationship, such as arial photography of multiple scenes.  Also preferred if you only care about aggregate data for the dataset, such as max occurrences of an object per scene.</p> Pipeline Use case <pre>arctic seal eo yolo</pre> detector for color imagery <pre>arctic seal ir yolo</pre> detector for infrared <pre>em tuna</pre> detector for identifying individual features of tuna <pre>fish without motion</pre> simple single-class fish detector <pre>generic proposals</pre> generic object detector <pre>motion</pre> detect regions of motion in video or time-series images <pre>pengcam kw</pre> Penguin cam full-frame classifier <pre>pengcam swfsc</pre> Penguin cam full-frame classifier <pre>scallop and flatfish</pre> detector for benthic images <pre>scallop and flatfish left</pre> detector for benthic images, process left half of each frame only <pre>scallop netharn</pre> deep learning detector for benthic images <pre>scallop netharn left</pre> deep learning detector for benthic images, process left half of each frame only <pre>sea lion multi class</pre> detects bulls, cows, pups, etc <pre>sea lion single class</pre> detector <pre>sefsc bw group</pre> black-and-white fish detector (18 class, lower granularity) (oldest, v1) <pre>sefsc bw species v2</pre> black-and-white fish species detector (updated, v2)"},{"location":"Pipeline-Documentation/#tracking","title":"Tracking","text":"<p>Run full tracking pipelines on your data.  Appropriate for videos and image sequences that derive from a video.  Tracking involves first running a detection pipeline then performing detection linking to form connected object tracks.</p> <p>Note some trackers can perform differently on time-series data depending on the annotation framerate selected when you upload or import your dataset. Higher framerates take longer to process, but may produce better results.</p> Pipeline Use case <pre>em tuna</pre> tracker <pre>fish</pre> simple fish tracker <pre>fish.sfd</pre> tracker <pre>generic</pre> generic object tracker puts generic boxes around arbitrary objects <pre>motion</pre> identifies moving object tracks <pre>mouss</pre> tracker, trained with data from MOUSS (Modular Optical Underwater Survey System) <pre>sefsc bw *</pre> same as above, but with tracking"},{"location":"Pipeline-Documentation/#utility","title":"Utility","text":"<p>An assortment of other types of utility pipelines.  Utility pipelines are named <code>utility_&lt;name&gt;.pipe</code> and are unique in that they may take detections as inputs (but are not required to).  </p> Pipeline Use case <pre>add segmentations watershed</pre> Transform existing bounding boxes into polygons <pre>empty frame lbls {N}fr</pre> Add an empty bounding box covering the whole media element for the purpose of adding full-frame classifier attributes. Unique tracks are created every N frames. <pre>track user selections</pre> Create tracks from user-initialized detection bounding boxes.  Draw a box on the first frame of a track, and the pipeline will continue tracking the selected object(s)"},{"location":"Pipeline-Documentation/#training","title":"Training","text":"<p>Run model training on ground truth annotations.  Currently, training configurations are available to do object detection, object classification, and full-frame classification.  Tracker training will be added in a future update.</p> <ul> <li>Full-frame classifiers can be trained on arbitrary multi-class labels.  It's helpful to start with <code>empty frame lbls</code> utility pipe and add type annotations to each generated frame.</li> <li>Object classifiers and detectors are trained on bounding boxes with arbitrary multi-class labels.</li> </ul>"},{"location":"Pipeline-Documentation/#overview","title":"Overview","text":"<ul> <li>SVM (Support Vector Machine) configurations are usable with the smallest amount of ground-truth and train relatively quickly.</li> <li>NetHarn is a pytorch deep learning framework that requires more input data: on the order of thousands of target examples.  There are two architectures used.  Netharn models can take up to several days to train.<ul> <li>Cascade Faster R-CNN (cfrnn) for training box detectors</li> <li>Mask R-CNN for training pixel classification and box detection</li> <li>ResNet (Residual Network) for training full frame or secondary object classifiers</li> </ul> </li> </ul>"},{"location":"Pipeline-Documentation/#options","title":"Options","text":""},{"location":"Pipeline-Documentation/#new-model-name","title":"New Model Name","text":"<p>A recognizable name for the pipeline that results from the training run.</p>"},{"location":"Pipeline-Documentation/#configuration-file","title":"Configuration File","text":"<p>One of the configuration options in the table below.</p>"},{"location":"Pipeline-Documentation/#labelstxt-file","title":"Labels.txt file","text":"<p>This optional file controls the output classes that a newly trained model will generate.</p> <ul> <li>Use if you annotated using higher granularity labels (such as species names) and want to train a classifier using more</li> <li>Or you want to restrict your training session to only train on certain kinds of ground-truth data.</li> </ul> <p>The following example <code>labels.txt</code> shows how to train a <code>FISH</code> classifier by combining <code>redfish</code> and <code>bluefish</code>, preserve the <code>ROCK</code> label, and omit every other label.</p> <pre><code>FISH redfish bluefish\nROCK\n</code></pre> <p>By default, all classes from all input datasets are preserved in the output model.</p>"},{"location":"Pipeline-Documentation/#use-annotation-frames-only","title":"Use annotation frames only","text":"<p>By default, training runs include all frames from the chosen input datasets, and frames without annotations are considered negative examples.  If you choose to use annotated frames only, frames or images with zero annotations will be discarded.  This option is useful for trying to train on datasets that are only partially annotated.</p>"},{"location":"Pipeline-Documentation/#configurations","title":"Configurations","text":"Configuration Availability Use Case detector_default both alias: train detector netharn cfrnn detector_netharn_cfrnn both detector_netharn_mask_rcnn both detector_svm_over_generic_detections both general purpose svm detector_svm_over_fish_detections both fish svm frame_classifier_default both alias: frame classifier netharn resnet frame_classifier_netharn_resnet both full-frame frame_classifier_svm_overn_resnet both full-frame object_classifier_default both alias: netharn resnet object classifier object_classifier_netharn_resnet both yolo desktop only can train, but resulting model is not runnable with desktop yet"},{"location":"Pipeline-Documentation/#pipeline-import-and-export","title":"Pipeline Import and Export","text":"<p>Pipelines created outside of VIAME Web can be upload and shared with other users.  See Pipeline Import and Export for details.</p>"},{"location":"Pipeline-Import-Export/","title":"Pipeline Import and Export","text":""},{"location":"Pipeline-Import-Export/#trained-model-downloads","title":"Trained model downloads","text":"<p>You can download your trained models through the administrative interface.</p> <p>Warning</p> <p>Use caution when modifying data through the admin interface</p> <ul> <li>Open the admin interface at https://viame.kitware.com/girder (or <code>myserver.com/girder</code> if you host your own instance)</li> <li> <p>Navigate to your personal workspace by clicking  My Folders under your user dropdown in the top right corner.</p> <p></p> </li> <li> <p>Navigate to the <code>VIAME/VIAME Training Results</code> folder and into the folder you wish to download</p> <p></p> </li> <li> <p>Select all items and download using the menu</p> <p></p> </li> </ul>"},{"location":"Pipeline-Import-Export/#custom-pipeline-upload","title":"Custom Pipeline Upload","text":"<p>It's possible to upload custom pipes to DIVE Web through the girder interface.</p> <p>Warning</p> <p>This feature is not yet standardized, and the instructions below may change.</p> <ol> <li>Open the girder interface at <code>/girder</code> and create a new private folder called <code>MyPipelines</code><ol> <li>For our demo instance, open https://viame.kitware.com/girder</li> </ol> </li> <li>Create a new folder in that private folder, and give it a name you'd like to associate with your new pipeline.</li> <li>Upload one or more files inside your new pipeline subfolder:<ol> <li>A pipeline file ending in the <code>.pipe</code> file extension</li> <li>Whatever other model <code>.zip</code> files are required by the pipe, named exactly as they appear in your <code>.pipe</code> file above.</li> </ol> </li> <li>Finally, set the pipeline folder metadata key <code>trained_pipeline</code> with value <code>true</code>.</li> <li>Your new pipeline will be available under the <code>Run Pipeline -&gt; Trained</code> menu from the DIVE web app.</li> </ol> <p></p>"},{"location":"Pipeline-Import-Export/#accepting-input","title":"Accepting input","text":"<p>If your pipe must accept input, set the pipeline folder metadata property <code>requires_input</code> to <code>true</code> .</p>"},{"location":"Pipeline-Import-Export/#including-base-pipelines","title":"Including base pipelines","text":"<p>User-uploaded pipelines may depend on any pipe already installed from the base image or an addon using <code>include &lt;pipename&gt;.pipe</code> . Depending on other user-uploaded pipes is not supported.</p> <p>Tip</p> <p>KWIVER pipe files can be exported for use with DIVE using kwiver pipe-config</p>"},{"location":"Screenshots/","title":"Screenshots","text":"<p>This page provides a general overview of the differences between desktop and web through screenshots.</p>"},{"location":"Screenshots/#browse-files","title":"Browse files","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#jobs-list","title":"Jobs List","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#annotator","title":"Annotator","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#training-config","title":"Training Config","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#settings","title":"Settings","text":"<p>Desktop</p> <p></p>"},{"location":"Support/","title":"Support","text":"<p>DIVE is free and open-source software published under an Apache 2.0 License in accordance with Kitware's Open Philosophy.</p>"},{"location":"Support/#community-support","title":"Community Support","text":"<p>For feedback, problems, questions, or feature requests, please reach out on Discourse. Our team would be happy to hear from you!</p> <p> Open a thread on Discourse</p>"},{"location":"Support/#advanced-support","title":"Advanced Support","text":"<p>If you or your employer have an active support contract with Kitware and you need individualized support, please email us directly at <code>viame-web@kitware.com</code>.</p> <p> Email us</p>"},{"location":"Support/#demos-or-custom-development","title":"Demos or Custom Development","text":"<p>DIVE is built and maintained by the Data &amp; Analytics group at Kitware with financial support from contracts with government and commercial customers.  If you would like to schedule a demo, sponsor custom software development, or purchase support related to DIVE, please email us or reach out through our contact page.</p>"},{"location":"UI-Annotation-View/","title":"Annotation Window","text":"<p>The annotation window will look different based on the current mode and what visibility toggles are enabled.</p> <ul> <li>Left Click an annotation to select it.</li> <li>Right Click an annotation to select it and enter editing mode.</li> <li>Middle Click and drag to pan the camera.  This is useful when drawing annotations while zoomed such that you need to work on something slightly off-screen.</li> </ul>"},{"location":"UI-Annotation-View/#viewer-modes","title":"Viewer modes","text":"<ul> <li> <p>Default Mode - In the default mode the annotation will have bounds associated with it as well as a text name for the type and an associated confidence level.  The color and styling will match what is specified in the Type List Style Settings.  There are additional modes which can be toggled on and off in the Edit Bar.</p> </li> <li> <p>Selected Annotation - selected annotations are cyan in color</p> <p></p> </li> <li> <p>Editing Annotation - Editing annotations are cyan in color and provide handles to resize the annotation as well as a central handle to move the annotation to different spot.</p> <p></p> </li> <li> <p>Creating Annotation - Creating an annotation requires clicking and dragging the mouse.  Creating in the annotation window is indicated by a cursor crosshair and an icon that shows the type being drawn.</p> <p></p> </li> <li> <p>Interpolated Annotation - If a track has an interpolated box on the current frame it will appear slightly faded.</p> <p></p> </li> </ul>"},{"location":"UI-AttributeConfiguration/","title":"Attribute Configuration","text":"<p>Attributes can be configured through their main editing window.  This provides several sections will allow for the customization of values.</p>"},{"location":"UI-AttributeConfiguration/#main","title":"Main","text":"<p>The Main section is where the attribute Name, Datatype, any predefined values and the rendering color are specified</p> <p>Datatype - Can either be Text, Number or Boolean data types.</p> <p>If the track is Text type a list of predefined values can be used in dropdowns to make it eaiser to set attribute values.</p> <p>Color - The color is used to determine the display color for the attribute within the track details and the default color utilized in any graphs or rendering of the attribute values.</p>"},{"location":"UI-AttributeConfiguration/#rendering","title":"Rendering","text":"<p>Attributes can be displayed within the annotation area next to the tracks they are associated with. Within the Attribute Editor there is a tab for Rendering and when turned on there are settings which can specify how the attribute is displayed and what tracks it is displayed. By default if no settings are touched the attributes are rendered below the bottom right of the track.</p> <p> In the above demo the Detection attributes are rendered to the side of the track with custom text and colors for displaying each.</p>"},{"location":"UI-AttributeConfiguration/#attribute-rendering-settings","title":"Attribute Rendering Settings","text":"<p>Under the Rendering Tab for the Attribute Editor if you turn on Render there will be numerous settings which determine how the attribute is displayed.</p> <ul> <li>Main Settings<ul> <li>Selected Track - only display attributes for the selected track.</li> <li>Filter Types - Will filter and only place the attribute rendering on the filtered track types.</li> <li>Order - Order is used to determine the top-to-bottom order of the attributes that are rendered.  A lower number means it has higher priority in the list.</li> </ul> </li> <li>Layout<ul> <li>Location - determines if the attribute is rendered inside or outside of the bounding box for the track</li> <li>Layout - Horizontal will render attributes as Name:Value left to right.  Vertical will stack them on top of each other.  Much like in the demo to the right side of track.</li> <li>Corner - SE/SW/NW specifies what corner to place the attributes in</li> </ul> </li> <li>Display Name<ul> <li>Display Name - The Name displayed at the top as a label for the attribute.  You can add a : to the display name.  It will automatically populate with the attribute name</li> <li>Display Text Size - Text size in pixel for the display name.  This will remain constant when scrolling in/out of the track. -1 is a default value to auto-calculate the size much like the confidence labels.</li> <li>Display Color - Text color for the display text.  If set to auto it will utilize the attribute color or the attribute value colors if they are specified.  If Auto is turned off you can set a custom display text color</li> </ul> </li> <li>Value<ul> <li>Value Text Size - Text size in pixel for the value.  This will remain constant when scrolling in/out of the track.</li> <li>Value Color - Text color for the display text.  If set to auto it will utilize the attribute color or attribute value colors if they are specified.  If Auto is turned off you can set a custom display text color.</li> </ul> </li> <li>Dimensions and Box Are only available under Vertical Layout</li> <li>Dimensions<ul> <li>% Type - For width and height it will size the area for the attribute based on the track width/height.</li> <li>px Type - It will size the dimension of the width/height in pixels.  This is useful if you have tracks of varying sizes and always want the attributes to fit properly.</li> <li>auto Type - Only for the height this will automatically partition the height of the track into even parts based on the number of attributes that are being used.</li> </ul> </li> <li>Box<ul> <li>Draw Box - Basic setting to draw the box, if not selected the attribute will float there.</li> <li>Thickness - Line Thickness for the outside of the box.</li> <li>Box Color - Line color for the box.  If set to auto it will utilize the attribute color.</li> <li>Box Background - Will draw a background for the box instead of being transparent</li> <li>Box Background Color - Background color for the box.  If set to auto it will utilize the attribute color.</li> <li>Box Background Opacity - The Opacity of the background color for the box</li> </ul> </li> </ul>"},{"location":"UI-AttributeConfiguration/#attribute-values","title":"Attribute Values","text":"<p>Attributes can specify specific render colors based on their values.  This means that attributes with text values could be set so that specific values will have specific colors.  I.E  You may have an attribute for weather classification and it could be set so that Sunny is Yellow, Cloudy is Gray and Rainy is Blue.</p>"},{"location":"UI-AttributeConfiguration/#attribute-text-values","title":"Attribute Text Values","text":"<p>On loading this page any existing attribute text values or predefined values will be populated and assigned a random color.  These colors can be clicked on and modified to suit needs.  If a text value doesn't exist, add it to the predefined values and it will show up when you return to this tab.</p> <p>When rendering attribute Values if the color is left to Auto it will utilize the colors specified here to render the text value for the attribute.</p> <p></p>"},{"location":"UI-AttributeConfiguration/#attribute-numerical-values","title":"Attribute Numerical Values","text":"<p>If the Attribute Datatype is 'Number' that allows for the creation of color gradients to be assigned to the numerical values. Numbers can be added by clicking on \"Add Color\".  Each Number in the range can then be specified to be a specific color.  Using D3.js it then creates a gradient between the colors in the list.  To help with visualization a display of the gradient is located at the bottom of the editor next to the Add Colors button.</p> <p>Like the Text Values if attribute rendering value is set to auto color it will use the gradient to determine the color of the attribute value.</p> <p></p>"},{"location":"UI-AttributeDetails/","title":"Attribute Details Panel","text":"<p>The Attribute Details Panel is a side panel that provides additional tools for Filtering and Viewing Attributes.</p> <p></p>"},{"location":"UI-AttributeDetails/#attribute-filtering","title":"Attribute Filtering","text":"<p>Filters the values of the selected type to reduce the number of results displayed.  The filters can be applied to  \"Track\" or \"Detection\" attributes and are applied in order.</p> <p>Attribute filters have an \"Applies To\" list which is a list of the Attribute key names that the filter will apply to. There is a special name called 'all' which will cause the filter to apply to all items. The settings for individual filters can be accessed by clicking on the </p>"},{"location":"UI-AttributeDetails/#numeric-filters","title":"Numeric Filters","text":"<p>only operates on the attributes that have numeric values.</p> <ul> <li>Range Filtering<ul> <li>Provides a slider with a custom range that can be used to filter the attribute values</li> <li>There is a Comparison option ('&gt;', '&lt;', '&gt;=', '&lt;=') which will filter out items that don't be the currently set number value</li> </ul> </li> <li>Top Filtering<ul> <li>Provides the Top {X} numbers when sorted by their numeric value</li> </ul> </li> </ul> <p></p>"},{"location":"UI-AttributeDetails/#string-filters","title":"String Filters","text":"<p>only operates on the attributes that have text values</p> <ul> <li>There are 4 options for text filtering: is, not, contains, starts<ul> <li>is - simple equality comparison to confirm that the value is equal to the test value.</li> <li>not - inverse of the equality expression.</li> <li>contains - will pass the filter if the item contains any of the values provided.</li> <li>starts - will pass the filter if the item starts with any of the values provided.</li> </ul> </li> </ul>"},{"location":"UI-AttributeDetails/#key-filters","title":"Key Filters","text":"<p>Filter which will only show the selected Attribute names regardless of their current value.</p> <p>The special name 'all' will show all of the attributes.</p>"},{"location":"UI-AttributeDetails/#timeline-visualization","title":"Timeline Visualization","text":"<p>The timeline visualization only applies to numeric and detection attributes currently.  It will graph the selected attributes in a chart at the bottom of the screen when a Track is selected.</p> <p>A Key Filter is used to determine which attributes to graph.</p> <p></p>"},{"location":"UI-AttributeTrackFiltering/","title":"Attribute Track Filtering","text":"<p>Attribute Track Filtering is used to apply a filter to the currently visible tracks based on the values defined in the Track or Detection Attributes</p>"},{"location":"UI-AttributeTrackFiltering/#adding-attribute-filters","title":"Adding Attribute Filters","text":"<p>To add attribute filters open up the right side panel and go to the drop down for Attribute Track Filters</p> <p></p> <p>A new Atribute Track Filter can be added by clicking on the new button</p> <p></p>"},{"location":"UI-AttributeTrackFiltering/#attribute-track-filter-options","title":"Attribute Track Filter Options","text":"<ul> <li>Filter Name - A displayed name for the track filter</li> <li>Track Types - A list of tracks that the filter will apply to.  If it is empty it will apply to all track types.</li> <li>Attribute Type - Track or Detection Attribute type</li> <li>Attribute - Dropdown list of the configured Track or Detection Attributes that is determined by the Attribute Type selection</li> <li>Enabled - The Filter is enabled by default.  This means on loading the filter is active and removing tracks that meet the condition</li> <li>Primary Display - Displays the filter in the left hand side of the primary interface.  Where the Confidence Filter and track types are listed</li> <li>User Editable - Instead of utilizing a static value, users can change the filter value to customize how it is filtering.  <ul> <li>If the Operator is a range slider this would include providing a slider for the user.</li> <li>If the Operator is a conditional (&gt;,&lt;,&gt;=,&lt;=,!=,==) or contains it will provide an input box for users to enter data</li> <li>If the Operator is 'in' for text it will include a multi dropdown to add in custom works to check for</li> </ul> </li> </ul> <p> Atribute Track Filter with Range Settings</p> <p>After adding multiple Track Attribute Filters they are listed in the side panel:</p> <p></p>"},{"location":"UI-AttributeTrackFiltering/#primary-track-atribute-filter-view","title":"Primary Track Atribute Filter View","text":"<p>All filters that have the Primary Display switched to on will be displayed in left hand side of the main viewer:</p> <p></p> <ul> <li>The Track Attribute Filter is collapsable and expandable.</li> <li>The Filter Icon  with the number next to it indicates the number of currently enabled filters.  This is useful if there are enabled filters that are not in the primary display.</li> <li>The Settings Icon   will open the Track Attribute Filter side panel directly for editing or modification of the existing filters.</li> <li>Each individual filter has a checkbox to enable/disable the filter.</li> <li>If the filter is User Editable there is an interface (range slider, input box, multi-select) to allow the user to change the filter values.</li> <li>Hovering over the info icon   will provide more information about the filter</li> </ul> <p></p>"},{"location":"UI-Attributes/","title":"Attributes","text":""},{"location":"UI-Attributes/#concepts-and-terms","title":"Concepts and Terms","text":"<ul> <li>Attribute Definitions are templates.  They have a name and a value type, such as <code>String</code>, <code>Number</code>, or <code>Boolean</code>.  Definitions must be created before attribute values can be assigned.  Tracks and detections each have their own set of definitions.</li> <li>Track Attributes apply to an entire track. Each track can only have one value for each track attribute definition.</li> <li>Detection Attributes can be different for every frame in a track.</li> </ul>"},{"location":"UI-Attributes/#example-attribute-definition","title":"Example Attribute Definition","text":"<ul> <li>Track Attributes:<ul> <li>CompleteTrack: <code>Boolean</code></li> <li>FishLength: <code>number (cm)</code></li> </ul> </li> <li>Detection Attributes:<ul> <li>Swimming: <code>Boolean</code></li> <li>Eating: <code>Boolean</code></li> </ul> </li> </ul>"},{"location":"UI-Attributes/#example-attribute-values","title":"Example Attribute Values","text":"<ul> <li>Fish Track 1<ul> <li>Track Attributes<ul> <li><code>{ \"FishLength\": 20 }</code></li> </ul> </li> <li>Detection Attributes<ul> <li>Frame 1<ul> <li><code>{ \"Eating\": True }</code></li> </ul> </li> <li>Frame 2<ul> <li><code>{ \"Swimming\": False, \"Eating\": True }</code></li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Info</p> <p>All Attribute definitions do not need to be assigned to values.  CompleteTrack (Track Attribute) and Swimming for Frame 1 (Detection Attribute) weren't assigned in this example.</p>"},{"location":"UI-Attributes/#using-the-attributes-panel","title":"Using the Attributes Panel","text":"<ol> <li>Select an existing track or detection with left click.</li> <li>Open the Track Details page by clicking on the  button in the Type List area or pressing the A key.</li> <li>Here you will see the track/detection type, confidence pairs associated with it and then a list of track and detection attributes.</li> <li>For attributes there are two sections<ol> <li>Track Attributes - All track level attributes</li> <li>Detection Attributes - attributes associated with the track on a per frame basis</li> </ol> </li> <li>Attributes can be sorted by their name (alphabetically) or by their numeric value.  Clicking on the  or the  button will swap between the two.</li> <li>Attribute Filtering<ol> <li>The Attribute filtering icon  will change color when filtering is being applied.</li> <li>Clicking on the filter icon will bring up the Attributes Details Panel</li> </ol> </li> </ol> <p>Info</p> <p>Attributes found during import in a VIAME CSV will automatically show up in the list.  The data type of the attribute is guessed by examining values and may need to be manually corrected.</p> <p>By default, all attributes associated with the dataset are visible and editable.  You can hide unused attributes by clicking the  toggle next to  Attribute.</p> Show Unused  Hide Unused"},{"location":"UI-Attributes/#creating-attribute-definitions","title":"Creating Attribute Definitions","text":"<ol> <li>Click on the  Attribute icon for in either the track or detection attribute area.<ol> <li></li> </ol> </li> <li>Enter a unique name</li> <li>Choose a Datatype<ol> <li><code>Number</code></li> <li><code>Boolean</code> (True/False)</li> <li><code>Text</code><ol> <li>Custom text that the user provides</li> <li>A predefined list of text to choose from, separated by newline.</li> </ol> </li> </ol> </li> <li>Click Save to add the new attribute</li> </ol>"},{"location":"UI-Attributes/#editing-attribute-definitions","title":"Editing Attribute Definitions","text":"<p>Click the  button next to an existing attribute to edit its definition.</p> <p></p> <p>Warning</p> <p>Editing or deleting an attribute definition doesn\u2019t change any existing attribute values.</p> <ul> <li>Deleting an attribute definition will cause it to disappear from the list, but the values will remain in the database.</li> <li>Editing an attribute definition will change the way the controls behave, but will not change any existing set values.</li> </ul>"},{"location":"UI-Attributes/#setting-attribute-values","title":"Setting Attribute Values","text":"<ol> <li>Click on the attribute value when in viewing mode to edit and set the attribute</li> <li>Or directly edit the value field when in the attribute editing mode</li> <li>Setting an attribute to the empty value will remove the value from the track/detection</li> </ol>"},{"location":"UI-Attributes/#importing-and-exporting-attributes","title":"Importing and Exporting Attributes","text":"<p>Attributes are part of the dataset configuration that can be imported and exported.</p> <ol> <li>Set up a dataset with all the attributes you need</li> <li>In the  Download menu, choose Configuration.</li> <li>Use this configuration with other datasets<ol> <li>Use the  Import button to load this configuration to other datasets.</li> <li>Upload the configuration file when you create new datasets to initialize them with these attribute definitions.</li> </ol> </li> </ol>"},{"location":"UI-Attributes/#applying-attributes-demo","title":"Applying Attributes Demo","text":""},{"location":"UI-Group-Manager/","title":"Group Manager","text":"<p>The group manager is one pane of the context sidebar.</p>"},{"location":"UI-Group-Manager/#feature-overview","title":"Feature overview","text":"<p>DIVE supports complex group annotation.</p> <ul> <li>Groups can be interpreted as activity participation.  For example, two <code>Person</code> type tracks participate in a <code>Conversation</code> activity.</li> <li>Groups can be used to represent most types of multi-annotation collections.</li> <li>Group membership for tracks can be explicitly constrained to sub-intervals within <code>[track.begin, track.end]</code>.  A single track can belong to a group for multiple sub-ranges, typically interpreted as the tracked object leaving and re-joining a group.</li> <li>Tracks can belong to many different groups at once, and can have different participation interval(s) for each.</li> </ul> <p>See the data format documentation for the complete capabilities of group annotations in the DIVE json schema.</p>"},{"location":"UI-Group-Manager/#group-list-controls","title":"Group List Controls","text":""},{"location":"UI-Group-Manager/#group-type-list","title":"Group Type List","text":"<p>The group type summary list allows for enabling and disabling annotations based on their group and bulk-editing group type characteristics.</p> <ul> <li>Groups do not have separate \"selected\" and \"editing\" states, so selecting a group puts it into editing mode.</li> <li>Group type styles can be edited in the same fashion as track type styles in the left sidebar using  (the edit pencil).</li> </ul>"},{"location":"UI-Group-Manager/#group-instance-list","title":"Group Instance List","text":"<p>Each group instance includes the following.</p> <ul> <li>A checkbox to enable and disable visibility</li> <li>The group id</li> <li>The group type</li> <li>A list of member track ids that are part of the group.</li> </ul>"},{"location":"UI-Group-Manager/#group-editor","title":"Group Editor","text":"<p>To enter group edit mode, click a group's ID number in the group list.</p> <ul> <li>Add new tracks to a group by first entering group edit mode, then selecting tracks to add (in the annotation window, the sidebar, or any UI where track selection can happen)</li> <li> (next to a track) will remove a track from a group.</li> <li>Use the frame range input boxes to adjust the start and end frame numbers that a track participates in a group.</li> <li> will set a frame input box to the current frame.</li> <li> will create a new sub-interval participation range for a track within a group.</li> <li> will remove a sub-interval participation range.</li> <li> Delete Group will delete a group without deleting its member tracks.</li> </ul> <p>Some notes about group editing behavior.</p> <ul> <li>If you delete a track, and the track was the only track remaining in one or more groups, those groups will also be deleted.</li> <li>If you delete a group, its member tracks will not be deleted no matter how many members there are.</li> <li>The group's composite range (shown as disabled begin and end frames in the group editor) is the maximum overlapping range of all sub-intervals of all member tracks.</li> </ul>"},{"location":"UI-Group-Manager/#example-data","title":"Example data","text":"<p>The group feature was initially developed for compatibility with the The Multiview Extended Video with Activities (MEVA) dataset.  Find example data at mevadata.org.</p>"},{"location":"UI-Navigation-Editing-Bar/","title":"Navigation and Editing Bar","text":""},{"location":"UI-Navigation-Editing-Bar/#navigation-bar","title":"Navigation Bar","text":"<p>The navigation bar is the row of controls at the very top of the window.</p> <ul> <li> Data navigates to the folder that contains the current dataset.</li> <li> Run Pipeline will launch a pipeline dropdown menu.<ul> <li>NOTE Current annotations will be replaced by the pipeline output when it is complete.  You should not perform annotations while a pipeline is running.</li> </ul> </li> <li> Import allows the upload of several kinds of files<ul> <li>overwrite the current annotations with a <code>.json</code> or <code>.csv</code> annotation file.</li> <li>overwrite the style and attribute configuration with a config <code>.json</code> file.</li> </ul> </li> <li> Download (Web) or  Export (Desktop) allows for exporting all or part of the current dataset.<ul> <li>Exclude Tracks - this allows you to remove tracks below a specific confidence threshold when exporting the CSV.  It is how you can export only the higher detections/tracks after running a pipeline.</li> <li>Checked Types Only - allows you to only export the annotations of types that are currently checked in the type list.</li> <li>Web-specific options are documented in the web download section</li> </ul> </li> <li> Clone is documented in the web clone section.</li> <li> Help provides mouse/keyboard shortcuts as well as a link to this documentation.</li> <li> is used to save outstanding annotation changes and any custom styles applied to the different types.  Changes are not immediately committed and will instead update the save icon with a number badge indicating how many changes are outstanding.  Clicking this button will commit your changes and reset the count to zero.</li> </ul>"},{"location":"UI-Navigation-Editing-Bar/#editing-bar","title":"Editing Bar","text":"<p>The editing bar is the second row below navigation.</p>"},{"location":"UI-Navigation-Editing-Bar/#editing-status-indicator","title":"Editing Status Indicator","text":"<p>On the far left, the editing mode status indicator shows you what mode you're in, what input is expected, and usually reminds you to press Esc to cancel.</p>"},{"location":"UI-Navigation-Editing-Bar/#edit-mode-toggles","title":"Edit Mode Toggles","text":"<p>Editing mode toggles control the type of geometry being created or edited during annotation.  See the Annotation Quickstart for an in-depth guide to annotation.</p>"},{"location":"UI-Navigation-Editing-Bar/#visibility-toggles","title":"Visibility Toggles","text":"<p>The  visibility section contains toggle buttons that control the different types of annotation data can be hidden or shown.</p> <ul> <li> toggles rectangle visibility</li> <li> toggles polygon visibility</li> <li> toggles head/tail line visibility</li> <li> toggles annotation type &amp; confidence text visibility</li> <li> toggles a cursor hover tooltip, helpful for reviewing very dense scenes with lots of overlap.</li> <li> toggles track trail visibility.  The track trail is configurable to show up to 100 frames both ahead and behind each bounding box.  The trail line is made of bounding box midpoints.</li> </ul>"},{"location":"UI-Timeline/","title":"Timeline","text":"<p>The timeline provides a control bar and a few different temporal visualizations.  All timeline visualizations are updated live by type confidence slider(s), type checkboxes, and track checkboxes.</p> <p></p> <p></p>"},{"location":"UI-Timeline/#control-bar","title":"Control Bar","text":"<ul> <li> will minimize the timeline.</li> <li># of Detections button selects the Track/Detection Count histogram timeline view.</li> <li>Events button selects the Event View, which is a Gantt-style track chart.</li> <li>Groups button selects the Group View, which is a Gantt-style group chart.</li> <li>  are standard media playback controls.</li> <li>  opens the video playback speed controls and volume controls, respectively.</li> <li>frame ## shows the current frame number.</li> <li> will enable camera lock, which causes the annotation view to auto-zoom and pan to whatever annotation is currently selected.  This is useful when reviewing the output of a pipeline.<ul> <li>Hovering over the camera lock will open additional settings for forcing transition and locking to a zoomed in multiple of the bbox size</li> </ul> </li> <li> or the R key will reset zoom/pan in the annotation view.</li> <li> will open the image contrast adjustment panel.</li> </ul>"},{"location":"UI-Timeline/#detectiontrack-count","title":"Detection/Track Count","text":"<ul> <li>Line color matches the annotation type style.</li> <li>Top green line is the sum count of all annotations of all types on each frame.</li> </ul> <p>This is the default visualization.  It is a stacked histogram of track/detection types over the duration of the sequence. Hovering over the Button for # of Tracks will bring up a settings panel where a user can swap between the # of Detections vs the # of Tracks * Tracks - This is the count of tracks at each frame which uses tracks begin/end times so tracks with gaps in the detections will still show up on the frame * Detections - This is a count of the detections so it will show gaps in tracks</p> <p></p> <ul> <li>Swap to Detections/Tracks will swap to the different styles of counting</li> <li>Show Total Count will hide/show the total count for all tracks/detections</li> </ul>"},{"location":"UI-Timeline/#event-view","title":"Event View","text":"<p>The event viewer shows the start/stop frames for all tracks.  It is a kind of Gantt chart, also similar to a swimlane chart but with more compact packing.</p> <ul> <li>The tracks are drawn using their corresponding type color.</li> <li>When hovering over any track the TrackID will display.</li> <li>Clicking on a track will select it and jump to the track at the selected frame.</li> </ul>"},{"location":"UI-Timeline/#group-view","title":"Group View","text":"<p>The group viewer is just like the event viewer, but shows the start and end times of track groups, colored by group types. Switching to the group view changes the coloring scheme of annotations in the annoation window.</p>"},{"location":"UI-Timeline/#interpreting","title":"Interpreting","text":"<p>Single frame detections are presented as single frames with spaces between.</p> <p> </p> <p>A selected track will be cyan and will cause all other tracks to fade out.  If a selected track is solid cyan, that means every frame in the track is a keyframe.</p> <p> </p> <p>A selected interpolated track will show the areas of interpolation as yellow lines, the keyframes as cyan ticks, and gaps as empty regions.</p>"},{"location":"UI-Track-List/","title":"Track List","text":""},{"location":"UI-Track-List/#track-list-controls","title":"Track List Controls","text":"<p>The track list allows for selecting and editing tracks.  A selected track will look different depending on whether it's a single detection or a multi-frame track.</p> <ul> <li> opens track creation settings</li> <li> deletes all tracks in the track list</li> <li> Track/Detection begins creation of a new annotation.</li> </ul>"},{"location":"UI-Track-List/#single-detection","title":"Single Detection","text":"<p>A track that spans a single frame.</p> <ul> <li> deletes the entire detection annotation</li> <li> goes to the first frame of the detection</li> <li> selects the detection and toggles edit mode.</li> </ul> <p>Warning</p> <p>The  button will remove the whole track if it's longer than a single detection.  To remove individual keyframes, use  (the keyframe toggle button).</p>"},{"location":"UI-Track-List/#multi-frame-track","title":"Multi-frame track","text":"<p>A track that spans multiple frames and has more options</p> <ul> <li> deletes the entire track</li> <li> splits the track into 2 smaller tracks on the current frame.</li> <li> is filled in if the current frame annotation is a keyfame.  Clicking this will either remove the keyframe if it exists or make the current interpolated annotation a keyframe.</li> <li> turns interpolation on/off for the interval between keyframes.</li> <li> jumps to the first frame of track</li> <li> jumps to the previous keyframe</li> <li> jumps to the next keyframe</li> <li> jumps to the last frame of the track</li> <li> selects the detection and toggles edit mode.</li> </ul>"},{"location":"UI-Type-List/","title":"Type List","text":""},{"location":"UI-Type-List/#type-list-controls","title":"Type List Controls","text":"<p>Each dataset maintains its own list of types, and types can be defined on-the-fly.</p> <p>The Type List is used to control visual styles of the different types as well as filter out types that don't need to be displayed.</p> <ul> <li>The checkbox next to each type name can be used to toggle types on and off.</li> <li> toggles the sort order between alphabetical and by number of annotations of each type.</li> <li> opens the type settings menu.</li> <li> will remove the type from any visible track or delete the track if it is the only type.</li> <li> will switch the left sidebar panel to show the track attribute editor (and group editor) view.</li> </ul>"},{"location":"UI-Type-List/#type-style-editor","title":"Type Style Editor","text":"<p>The type style editor controls the visual appearance of annotations in all other areas of the application.  Launch the editor by hovering over a type row in the list and clicking  (the edit pencil).</p> <ul> <li>Type Name - You can change the name for the type and it will update all subsequent tracks that are using that Type.</li> <li>Show Label - show the type name label in the text above each box.</li> <li>Show Confidence - show the confidence value in the text above each box.</li> <li>Box Border Thickness - the line thickness can be changed to make a type stand out more or less</li> <li>Fill - Fill allows the bounding box to be filled.  This is useful for annotation of background items in an image.</li> <li>Border &amp; Fill Opacity - The opacity of the lines and the fill can be set here.</li> <li>Color - The color for the type within the annotations and the timeline views.</li> </ul>"},{"location":"UI-Type-List/#type-settings-menu","title":"Type Settings Menu","text":"<p>Click the  button in the type list heading to open type settings.</p>"},{"location":"UI-Type-List/#ad-hoc-mode","title":"Ad-hoc mode","text":"<p>In ad-hoc mode, new object classes are added as you annotate.  The type list updates automatically when new classes are added or the last member of a class is deleted.</p> <ul> <li>Set Lock Types to off for ad-hoc type creation.</li> <li>Set Show Empty to still show manually defined types with no track/detection examples in the type list.</li> </ul>"},{"location":"UI-Type-List/#locked-mode","title":"Locked mode","text":"<p>In locked mode, only a specified list of classes can be used, and must be selected or autocompleted from the list for each object.</p> <ul> <li>Set Lock Types to on to constrain annotation types to those already defined.</li> <li>You can add new types using the  Types button under type settings.</li> </ul>"},{"location":"Web-Version/","title":"Web Version","text":"<p>Info</p> <p>VIAME Web is automatically updated at 2AM EST/EDT every Thursday.  If you are running a pipeline or training workflow during update, it will be interrupted and restarted.</p> <p>Also note that pipelines and training jobs on our public server are limited to 3 days of execution time on 1 GPU.  If you have a job that needs more time, please run it with the Desktop version, your own cloud environment like GCP, or contact us for support.</p> <p>Use our public server  Deploy your own</p>"},{"location":"Web-Version/#register-for-an-account","title":"Register for an account","text":"<p>A user account is required to store data and run pipelines on viame.kitware.com.</p> <ol> <li>Visit viame.kitware.com</li> <li>Click Register</li> </ol>"},{"location":"Web-Version/#uploading-data","title":"Uploading data","text":""},{"location":"Web-Version/#uploading-individual-files","title":"Uploading individual files","text":"<ul> <li>Open the DIVE Homepage, and navigate to the  Data tab.</li> <li>Click the  User Home button at the top left of the data browser.</li> <li>Click either your  Public or  Private folder, or make a new folder and navigate into it.</li> <li>Click the  Upload button that appears in the toolbar.</li> <li>Select a video or multi-select a group of image frames.<ul> <li>Use Ctrl or Shift to click every file you want to upload.</li> <li>If you already have <code>annotations.csv</code> or an annotation or configuration JSON select that too.</li> </ul> </li> <li>Choose a name for the dataset and enter the optional playback frame rate or select other optional files.</li> <li>Press Start Upload</li> <li>In the data browser, a new Launch Annotator button will appear next to your data<ul> <li>If you uploaded a video, it may need to transcode first</li> </ul> </li> </ul> <p>Info</p> <p>All video uploaded to the web server will be transcoded as <code>mp4/h264</code>.</p>"},{"location":"Web-Version/#uploading-zip-files","title":"Uploading zip files","text":"<p>A zip import can have one of the following file combinations:</p> <ul> <li>One or more images, an optional annotation file, and an optional configuration file</li> <li>One video with an optional annotation file and an optional configuration file</li> <li>One or more folders which contain the above examples (These will be converted to separate datasets)</li> </ul> <p>Zip import also accepts zip archive files that were generated by the Download Everything export button.</p>"},{"location":"Web-Version/#download-or-export-data","title":"Download or export data","text":"<p>Data can be downloaded from the FileBrowser by clicking the checkmark to the left of a dataset name.  This allows you to download the source images/video, the current detection file converted to <code>.csv</code> or everything including all backups of the detection files.</p> <ul> <li>Image Sequence or Video will export the source media as a <code>.zip</code></li> <li>Detections will export a VIAME <code>.csv</code> of annotations<ul> <li>Checkbox options are explained in the Navigation Bar Section.</li> </ul> </li> <li>Configuration will export a DIVE configuration <code>.json</code></li> <li>Everything will export all of the above.</li> </ul>"},{"location":"Web-Version/#sharing-data-with-teams","title":"Sharing data with teams","text":"<p>This information will be relevant to teams where several people need to work on the same data.</p>"},{"location":"Web-Version/#concepts","title":"Concepts","text":"<p>By default, data uploaded to your personal user space follows these conventions.</p> <ul> <li>Data in the  Public folder is readable by all registered users, but writable only by you by default.</li> <li>Data in the  Private folder is only visible to you by default.</li> </ul>"},{"location":"Web-Version/#working-with-teams","title":"Working with teams","text":"<p>A common scenario is for a group to have a lot of shared data that several members should be able to view and annotate.</p> <p>For most teams, we recommend keeping data consolidated under a single account then following the sharing instructions below to make sure all team members have appropriate access.</p> <p>It's easiest to create a single parent folder to share and then put all individual datasets inside that parent.</p> <p>Warning</p> <p>You should note that 2 people cannot work on the same video at the same time.  Your team should coordinate on who will work on each dataset.</p>"},{"location":"Web-Version/#managing-permissions","title":"Managing Permissions","text":"<p>DIVE uses Girder's Permissions Model.</p> <p>There are four levels of permission a User can have on a resource.</p> <ul> <li>No permission (cannot view, edit, or delete a resource)</li> <li>READ permission (can view and download resources)</li> <li>WRITE permission (includes READ permission, can edit the properties of a resource)</li> <li>ADMIN also known as own permission, (includes READ and WRITE permission, can delete the resource and also control access on it)</li> </ul>"},{"location":"Web-Version/#granting-access-to-others","title":"Granting access to others","text":"<ul> <li>Navigate to your data in the data browser.</li> <li> <p>Right click a dataset or a folder of datasets and choose Access Control</p> <p></p> </li> <li> <p>Search for and select users you want to grant access to.</p> </li> <li> <p>Select the correct permissions in the drop-down next to each user.</p> <p></p> </li> <li> <p>If this is a folder of datasets, enable the Include Subfolders switch.</p> </li> <li>Click Save.  These users should now be able to view and edit your data.</li> </ul>"},{"location":"Web-Version/#data-shared-with-you","title":"Data Shared with you","text":"<p>You can view data shared with you by selecting the  Shared With Me tab above the data browser.</p> <p></p>"},{"location":"Web-Version/#sharing-urls","title":"Sharing URLs","text":"<p>You can copy and paste any URL from the address bar and share with collaborators.  This includes folders in the data browser as well as direct links to the annotation editor.</p>"},{"location":"Web-Version/#dataset-clones","title":"Dataset Clones","text":"<p>A clone is a shallow copy of a dataset.</p> <ul> <li>It has its own annotations, and can be run through pipelines and shared with others.</li> <li>It references the media (images or video) of another dataset.</li> </ul> <p>Warning</p> <p>Be careful when deleting data that has been cloned.  Clones \"point to\" their source dataset for loading media, so if the source is deleted, all of its clones will fail to load.</p>"},{"location":"Web-Version/#clone-use-cases","title":"Clone use cases","text":"<ol> <li>When you want to use or modify data that doesn't belong to you, such as data from the shared training collection or from other users.</li> <li>When you want to run several different pipelines in parallel on the same input data and compare the results.</li> </ol> <p>Warning</p> <p>Merging cloned data back to the source is not currently supported.  To collaborate with others on annotations, the sharing use case above is preferred.</p>"},{"location":"Web-Version/#how-to-clone","title":"How to clone","text":"<ul> <li>Open the dataset you wish to clone by clicking Launch Annotator.</li> <li>Click the  Clone button in the top navigation bar on the right side.</li> <li>Choose a name and location for the clone within your own workspace.</li> </ul>"},{"location":"Web-Version/#revision-history","title":"Revision History","text":"<p>Revision history is accessible through the annotation UI in the Web version.  Each time you press  Save, a new revision of your annotation state is created.  It is possible to inspect (or \"check out\") past revisions.  The viewer will be in read-only mode when past revisions are checked out because only the most recent revision can be modified.</p> <ul> <li>Click  History in the Navigation Bar area to open the Revision History panel.<ul> <li>Each row shows the revision datetime, the action that caused it, and the number of additions and deletions.</li> <li>Click a row to check out a previous revision</li> </ul> </li> <li>Click  Download when a previous revision is checked out to download the annotation CSV from that revision.</li> <li>Click  Clone when a previous revision is checked out to create a new clone of the dataset from that revision.</li> </ul> <p>Info</p> <p>Revision roll-back is not yet supported, but will be added in a future update.  If you need to roll-back to a previous version of your anotation state</p> <ul> <li>check out the old version and create a CSV download, then re-upload the older version using import;</li> <li>or contact us for support.</li> </ul>"},{"location":"scripting/Endpoints/","title":"GirderClient and DIVE REST Endpoints","text":"<p>DIVE can be interacted with programatically utizing both Girder and DIVE endpoints to upload data/download data modify annotations and run pipelines/training.</p>"},{"location":"scripting/Endpoints/#main-dive-endpoints","title":"Main DIVE Endpoints","text":"<p>Going the the <code>{URL}/api/v1</code> like viame.kitware.com/api/v1 will provie a Swagger description of the all of the Girder endpoints</p>"},{"location":"scripting/Endpoints/#dive_dataset","title":"dive_dataset/","text":"<p>Operating directly on  DIVE datasets for higher level information about the media, attributes and configuration.</p>"},{"location":"scripting/Endpoints/#dive_dataset_1","title":"<code>dive_dataset/</code>","text":"<ul> <li>Method: GET</li> <li>Usage: List DIVE datasets in the system that the current user has access to.  By default it uses a limit of 50 to prevent listing a large number of datasets</li> </ul>"},{"location":"scripting/Endpoints/#dive_dataset_2","title":"<code>dive_dataset/</code>","text":"<ul> <li>Method: POST</li> <li>Usage: This endpoint is used to create a new dataset in the system, by generating a clone of an existing DIVE dataset.</li> </ul>"},{"location":"scripting/Endpoints/#dive_datasetexport","title":"<code>dive_dataset/export</code>","text":"<ul> <li>Method: GET</li> <li>Usage: This endpoint is used to export the entire dataset, including annotations and media files, in a specified format.  It will export the data in a zip file.  The input parameter of folderIds in an array of DIVE dataset FolderIds</li> </ul>"},{"location":"scripting/Endpoints/#dive_datasetidconfiguration","title":"<code>dive_dataset/{id}/configuration</code>","text":"<ul> <li>Method: GET</li> <li>Usage: Gets the configuration for the DIVE dataset Id and returns a JSON file for downloading that contains the configuration options/</li> </ul>"},{"location":"scripting/Endpoints/#dive_annotation","title":"dive_annotation/","text":"<p>Operating on the Annotations for a specific DIVE Dataset Id.  These incloude getting and modification of the DIVE Dataset annotation values.</p>"},{"location":"scripting/Endpoints/#dive_annotationtrack","title":"<code>dive_annotation/track</code>","text":"<ul> <li>Method: GET</li> <li>Usage: This endpoint is used to get detailed information about specific tracks within a dataset, including their attributes and associated detections.  There are options to retrieve the annotations at specific revisions</li> </ul>"},{"location":"scripting/Endpoints/#dive_annotationrevision","title":"<code>dive_annotation/revision</code>","text":"<ul> <li>Method: GET</li> <li>Usage: This endpoint is used to access the list of revisions for annotations.  I.E everytime a user modified the annotations through a pipeline or through saving changes</li> </ul>"},{"location":"scripting/Endpoints/#dive_annotationrollback","title":"<code>dive_annotation/rollback</code>","text":"<ul> <li>Method: POST</li> <li>Usage: Rolls back the Annotations to a specific revision version</li> </ul>"},{"location":"scripting/Endpoints/#dive_annotation_1","title":"<code>dive_annotation</code>","text":"<ul> <li>Method: PATCH</li> <li>Usage: This endpoint is used to modify existing annotations, such as updating track information or adding new attributes.</li> </ul>"},{"location":"scripting/Endpoints/#dive_annotationexport","title":"<code>dive_annotation/export</code>","text":"<ul> <li>Method: GET</li> <li>Description: Exports annotations for a given dataset.</li> <li>Usage: This endpoint is used to export annotations in a specified format (e.g., CSV, JSON) for a dataset.  This endpint is different from dive_annotation/track because it returns a file rather than direct JSON like dive_annotation/track does.</li> </ul>"},{"location":"scripting/Endpoints/#dive_rpc","title":"dive_rpc/","text":"<p>These are remote procedural calls to run jobs or perform actions that may be a bit longer running than simple request.  This is where pipelines and training will be run or the initial transcoding for videos/images can be kicked off.</p>"},{"location":"scripting/Endpoints/#dive_rpcpostprocessid","title":"<code>dive_rpc/postprocess/{id}</code>","text":"<ul> <li>Method: POST</li> <li>Usage: This endpoint is used to trigger postprocessing tasks on a dataset.  It is a requirement that after new data is uploaded this endpoint is called to transcode data and process any uploaded CSV or JSON files to generate attributes and the base annotations.  After uploading any data this endpoint should be called with <code>skipJobs = True</code> to process the annotation file and update the attributes.</li> </ul>"},{"location":"scripting/Endpoints/#dive_rpcpipeline","title":"<code>dive_rpc/pipeline</code>","text":"<ul> <li>Method: POST</li> <li>Usage: This endpoint is used to execute a specified pipeline on a dataset, which can include tasks like object detection, tracking, and classification.</li> </ul>"},{"location":"scripting/Endpoints/#dive_rpctrain","title":"<code>dive_rpc/train</code>","text":"<ul> <li>Method: POST</li> <li>Usage: This endpoint is used to train a machine learning model using the annotations and media in a dataset, allowing for the creation of custom models for specific tasks.</li> </ul>"},{"location":"scripting/GirderClient/","title":"Girder Client","text":"<p>Girder Client is a python client library that makes it eaiser to work with Girder/DIVE endpoints.</p> <p>Girder Client Documentation</p>"},{"location":"scripting/GirderClient/#initialization","title":"Initialization","text":"<pre><code>apiURL = \"viame.kitware.com\" # can also use localhost for local development\nport = 443 # 8010 for local development\ndef login():\n    gc = girder_client.GirderClient(apiURL, port=port, apiRoot=\"girder/api/v1\")\n    gc.authenticate(interactive=True)\n    return gc\n</code></pre> <p>This code snippet creates a login context for girder-client.  By interactively in the python script asking for the username and password.  If you want to utilize an apiKey you can use the following instead of <code>interactive=True</code> asking for the username/password:</p> <pre><code>gc.authenticate(apiKey=apiKeyVar)\n</code></pre> <p>The apiKey can be accessed by going to the girder-endpoint <code>/token/current</code> if you are logged into the system.  I.E for viame.kitware.com this would be viame.kitare.com/api/v1/token/current</p>"},{"location":"scripting/GirderClient/#common-girder-client-functions","title":"Common Girder Client Functions","text":""},{"location":"scripting/GirderClient/#girder_clientgetpath-parametersnone-jsonresptrue","title":"girder_client.get(path, parameters=None, jsonResp=True)","text":"<p>Function used to send a simplified GET request to a specific endpoint path like <code>/dive_dataset</code>. The paramemters can be entered as a dictionary of values The jsonResp defaults to true.  If an endpoint returns binary data or data that is not JSON set jsonResp to False.</p>"},{"location":"scripting/GirderClient/#girder_clientsendrestrequestmethod-path-parametersnone-datanone-filesnone-jsonnone-headersnone-jsonresptrue","title":"girder_client.sendRestRequest(method, path, parameters=None, data=None, files=None, json=None, headers=None, jsonResp=True)","text":"<p>Sends a specific Method: (GET, PATCH, DELETE, POST...) to the path with the parameters and possible files.  This allows for more fine grained control of the REST request send to the endpoint </p>"},{"location":"scripting/GirderClient/#girder_clientlistfolderparentid-parentfoldertypefolder-namenone-limitnone-offsetnone","title":"girder_client.listFolder(parentId, parentFolderType='folder', name=None, limit=None, offset=None)","text":"<p>Given a parentId folder this will list all of the folders based on the other filter parameters.  This is useful to list subfolders within a parent folder.</p>"},{"location":"scripting/GirderClient/#girder_clientaddmetadatatofolderfolderid-metadata","title":"girder_client.addMetadataToFolder(folderId, metadata)","text":"<p>Allows adding metadata to a specific folder.  This can be usefule to change attributes specifications for a folder or mark a folder as 'annotate=True' to indicate that it is a DIVE Dataset folder.</p>"},{"location":"scripting/GirderClient/#girder_clientuploadfiletofolderfolderid-filepath-referencenone-mimetypenone-filenamenone-progresscallbacknone","title":"girder_client.uploadFileToFolder(folderId, filepath, reference=None, mimeType=None, filename=None, progressCallback=None)","text":"<p>Uploads a specific filder to a parent folder.  This would probably be used in conjunction with girder_client.sendRestRequest('POST', 'dive_rpc/postprocess/{id}', parameters= {'skipJobs': True, 'skipTranscoding': True}) where the Id is the folderId.  This way you can upload a CSV/JSON annotation file to a folder then call postprocess to process that data</p>"},{"location":"scripting/Scripting/","title":"Scripting","text":"<p>The data managment system through DIVE and Girder may not be enough for very large datasets or complicated datasets.  Scripting the uploading/downloading and processing of data may become necessary.  This can be done utilizing the DIVE/Girder Rest Endpoints and a Python pacakge called 'GirderClient' that helps with interfacing these endpoints.</p>"},{"location":"scripting/Scripting/#endpoints","title":"Endpoints","text":"<p>The endpoints documentation provides a more comprehensive list of commonly used endpoints when scripting.  These are endpoints for uploading/downloading data as well as running postprocess, pipelines and training on DIVE Datasets.</p>"},{"location":"scripting/Scripting/#girderclient","title":"GirderClient","text":"<p>The documentation for GirderClient and some introductury scripts for authentication along with some commonly used functions for DIVE Scripting</p> <p>There are several example scripts provided in the repository including:</p> <ul> <li>userCount.py - A script utilized by admins to download information about all of the datasets in the system and determine the ttoal number of users that are avaialable.</li> <li>setAnnotationFPS.py - Sets the annotation FPS on a sample folder by modification of the metdata on the DIVE Dataset Folder</li> <li>uploadScript - The process of uploading a new JSON or ViameCSV formatted file to a DIVE Dataset folder and running the <code>postprocess</code> endpoint to add these new annotations.</li> <li>syncAnnotationsScript.py - Script to sync a folder hierarchy of Annotation files with a similar folder structur within DIVE</li> <li>exportAnnotations.py - Script that takes in a list of base FolderIds, recursively finds the DIVE Datasets under the hierarchy and then downloads all annotations to a folder structure mimicing the girder structure.</li> </ul>"}]}